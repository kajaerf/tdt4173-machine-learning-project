{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycaret.regression import *\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_a = pd.read_parquet('A/train_targets.parquet')\\ntrain_b = pd.read_parquet('B/train_targets.parquet')\\ntrain_c = pd.read_parquet('C/train_targets.parquet')\\n\\nX_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\\nX_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\\nX_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\\n\\nX_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\\nX_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\\nX_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\\n\\nX_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\\nX_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\\nX_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "def preprocessing(targets, observed, estimated, test):\n",
    "   \n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    # Resample observed, estimated, and test data to 1 hour using mean() as aggregator\n",
    "    # and drop rows where all columns are NaN\n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    # Merge the observed and estimated data\n",
    "    weather_data = pd.concat([observed_resampled, estimated_resampled])\n",
    "\n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Drop non-feature columns\n",
    "    merged_data = merged_data.drop(columns=['time', 'date_forecast', 'pv_measurement'])\n",
    "    \n",
    "    return merged_data, test_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "def process_location(X, y, location_name):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "\n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=123,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx'],\n",
    "                    html=False,\n",
    "                    #silent=True,  # It's important to set silent=True to avoid PyCaret's interactive input requests\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "\n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model - this will train it on the complete dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from pycaret.time_series import *\\ndef process_location_time(X, y, location_name):\\n    data = X.copy()\\n    data['target'] = y\\n    data.dropna(axis=1,inplace=True)\\n    exp_reg = setup(data=data, target='target', session_id=123,\\n                    #categorical_features=['dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx'],\\n                    html=False,\\n                    #silent=True,  # It's important to set silent=True to avoid PyCaret's interactive input requests\\n    experiment_name=f'exp_{location_name}')\\n    best = compare_models(include=['dt_cds_dt', 'rf_cds_dt','auto_arima' ])\\n    tuned_best = tune_model(best)\\n    #bagged_best = ensemble_model(tuned_best, method='Bagging')\\n    final_model = finalize_model(tuned_best)\\n    save_model(final_model, f'final_model_for_location_{location_name}')\\n    return final_model\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from pycaret.time_series import *\n",
    "def process_location_time(X, y, location_name):\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "    data.dropna(axis=1,inplace=True)\n",
    "    exp_reg = setup(data=data, target='target', session_id=123,\n",
    "                    #categorical_features=['dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx'],\n",
    "                    html=False,\n",
    "                    #silent=True,  # It's important to set silent=True to avoid PyCaret's interactive input requests\n",
    "    experiment_name=f'exp_{location_name}')\n",
    "    best = compare_models(include=['dt_cds_dt', 'rf_cds_dt','auto_arima' ])\n",
    "    tuned_best = tune_model(best)\n",
    "    #bagged_best = ensemble_model(tuned_best, method='Bagging')\n",
    "    final_model = finalize_model(tuned_best)\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "    return final_model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"locations = ['A', 'B', 'C']\\nall_models = []\\n\\nfor loc in locations:\\n    # Load your data\\n    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\\n    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\\n    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\\n    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\\n\\n    # Preprocess data\\n    X_train, X_test = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\\n    y_train = train['pv_measurement'].values\\n\\n    # Ensure X and y have the same length\\n    min_length = min(len(X_train), len(y_train))\\n    X_train, y_train = X_train.iloc[:min_length], y_train[:min_length]\\n\\n    # Process the location data and get the final model\\n    final_model = process_location(X_train, y_train, loc)\\n\\n    # Store the model in all_models list\\n    all_models.append(final_model)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''locations = ['A', 'B', 'C']\n",
    "all_models = []\n",
    "\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    y_train = train['pv_measurement'].values\n",
    "\n",
    "    # Ensure X and y have the same length\n",
    "    min_length = min(len(X_train), len(y_train))\n",
    "    X_train, y_train = X_train.iloc[:min_length], y_train[:min_length]\n",
    "\n",
    "    # Process the location data and get the final model\n",
    "    final_model = process_location(X_train, y_train, loc)\n",
    "\n",
    "    # Store the model in all_models list\n",
    "    all_models.append(final_model)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Description        Value\n",
      "0                    Session id          123\n",
      "1                        Target       target\n",
      "2                   Target type   Regression\n",
      "3           Original data shape  (34061, 46)\n",
      "4        Transformed data shape  (34061, 62)\n",
      "5   Transformed train set shape  (23842, 62)\n",
      "6    Transformed test set shape  (10219, 62)\n",
      "7              Numeric features           42\n",
      "8          Categorical features            3\n",
      "9      Rows with missing values        97.4%\n",
      "10                   Preprocess         True\n",
      "11              Imputation type       simple\n",
      "12           Numeric imputation         mean\n",
      "13       Categorical imputation         mode\n",
      "14     Maximum one-hot encoding           25\n",
      "15              Encoding method         None\n",
      "16               Fold Generator        KFold\n",
      "17                  Fold Number           10\n",
      "18                     CPU Jobs           -1\n",
      "19                      Use GPU        False\n",
      "20               Log Experiment        False\n",
      "21              Experiment Name        exp_A\n",
      "22                          USI         7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MAE          MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                         \n",
      "0     202.0913  182170.5580  426.8144  0.8672  1.2504  1.3275\n",
      "1     192.8785  175765.0472  419.2434  0.8669  1.2242  1.2884\n",
      "2     188.3806  163246.3219  404.0375  0.8769  1.2707  1.5085\n",
      "3     187.7374  162585.8416  403.2193  0.8873  1.2525  1.4801\n",
      "4     199.6796  193202.7568  439.5484  0.8585  1.3544  1.8367\n",
      "5     203.2024  210479.8879  458.7809  0.8444  1.3148  1.5133\n",
      "6     197.9293  201171.7252  448.5217  0.8550  1.3122  1.7340\n",
      "7     192.8957  169295.8879  411.4558  0.8771  1.1906  1.2148\n",
      "8     199.7484  180669.0362  425.0518  0.8698  1.2225  1.2588\n",
      "9     199.9936  210477.7095  458.7785  0.8466  1.3494  1.2449\n",
      "Mean  196.4537  184906.4772  429.5452  0.8650  1.2742  1.4407\n",
      "Std     5.2904   17208.5569   19.9353  0.0131  0.0533  0.2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MAE          MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                         \n",
      "0     203.0405  186466.8836  431.8181  0.8641  1.3173  1.7052\n",
      "1     195.4510  178940.7393  423.0139  0.8645  1.3483  1.2889\n",
      "2     188.6688  168772.1163  410.8188  0.8727  1.3262  1.8837\n",
      "3     184.3176  154213.0842  392.6997  0.8931  1.2919  1.9547\n",
      "4     197.7693  192621.7028  438.8869  0.8589  1.3804  2.1457\n",
      "5     200.4196  201603.8951  449.0032  0.8510  1.4109  1.6544\n",
      "6     190.5849  184500.3472  429.5350  0.8670  1.3475  1.7576\n",
      "7     198.1652  178601.7667  422.6130  0.8703  1.3573  1.3687\n",
      "8     203.4899  183101.1893  427.9032  0.8681  1.3139  1.3808\n",
      "9     201.9291  209483.1905  457.6933  0.8474  1.3560  1.3943\n",
      "Mean  196.3836  183830.4915  428.3985  0.8657  1.3450  1.6534\n",
      "Std     6.2173   14880.7463   17.4697  0.0120  0.0329  0.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "locations = ['A', 'B', 'C']\n",
    "all_predictions = []\n",
    "final_df_list = [] \n",
    "\n",
    "\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    y_train = train['pv_measurement'].values\n",
    "\n",
    "    # Ensure X and y have the same length\n",
    "    min_length = min(len(X_train), len(y_train))\n",
    "    X_train, y_train = X_train.iloc[:min_length], y_train[:min_length]\n",
    "\n",
    "    # Process the location data and get the final model\n",
    "    final_model = process_location(X_train, y_train, loc)\n",
    "    X_test.dropna(axis=1,inplace=True)\n",
    "    # Predict on new data\n",
    "    predictions = predict_model(final_model, fh=len(X_test), X=X_test)\n",
    "\n",
    "    print(predictions.columns)\n",
    "\n",
    "    # Store predictions\n",
    "    all_predictions.append(predictions['y_pred'])  \n",
    "\n",
    "    # Prepare dataframe for visualization\n",
    "    num_predictions = len(predictions)\n",
    "    timestamps = pd.date_range(start='2023-05-01 00:00:00', periods=num_predictions, freq='H')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'time': timestamps,\n",
    "        'prediction': predictions['y_pred'],\n",
    "        'location': loc\n",
    "    })\n",
    "\n",
    "    final_df_list.append(df)\n",
    "\n",
    "# Concatenate all the individual data frames and prepare for plotting\n",
    "final_df = pd.concat(final_df_list, ignore_index=True)\n",
    "final_df['time'] = pd.to_datetime(final_df['time'])\n",
    "final_df.sort_values('time', inplace=True)\n",
    "\n",
    "# Visualization\n",
    "for loc in locations:\n",
    "    # Filter data for each location\n",
    "    temp_df = final_df[final_df['location'] == loc]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Create a new figure for each location\n",
    "    plt.plot(temp_df['time'], temp_df['prediction'], label=f'Location {loc}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'Predictions Over Time for Location {loc}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# If you need to save the final dataframe without 'time' and 'location' columns, you can do so before plotting\n",
    "final_df_save = final_df.copy()\n",
    "final_df_save = final_df_save.drop(columns=['time', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)'''\n",
    "\n",
    "final_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('final_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
