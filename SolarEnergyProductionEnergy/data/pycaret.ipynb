{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycaret.regression import *\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TIME FEATURES\n",
    "def add_time_features(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "    \n",
    "    # Extract various time features\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_column].dt.isocalendar().week \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "def preprocessing(targets, observed, estimated, test):\n",
    "   #bruk lasso regularisering\n",
    "   #annen learing rate, 0.0001\n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    # Resample observed, estimated, and test data to 1 hour using mean() as aggregator\n",
    "    # and drop rows where all columns are NaN\n",
    "    targets = targets.set_index('time').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    test_resampled = test_resampled.drop('is_day:idx', axis=1)\n",
    "    observed_resampled = observed_resampled.drop('is_day:idx', axis=1)\n",
    "    estimated_resampled = estimated_resampled.drop('is_day:idx', axis=1)\n",
    "    \n",
    "    if observed_resampled.empty:\n",
    "      print(f\"observed_resampled is empty for location \")\n",
    "\n",
    "\n",
    "   \n",
    "    # Merge the observed and estimated data\n",
    "    weather_data = pd.concat([observed_resampled, estimated_resampled])\n",
    "\n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Add the time-based features\n",
    "    merged_data = add_time_features(merged_data, 'time')  \n",
    "    test_resampled = add_time_features(test_resampled, 'date_forecast') \n",
    "    if merged_data.empty:\n",
    "      print(f\"merged_data is empty for location \")\n",
    "    merged_data = merged_data[merged_data['pv_measurement'] != 0]\n",
    "    targets = merged_data['pv_measurement']\n",
    "    # Drop non-feature columns\n",
    "    merged_data = merged_data.drop(columns=['time', 'date_forecast', 'pv_measurement'])\n",
    "    \n",
    "    return merged_data, test_resampled, is_day_feature, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "def process_location(X, y, location_name):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "\n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=123,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx'],\n",
    "                    \n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False,\n",
    "                    #silent=True,  \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "\n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model - this will train it on the complete dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18038 X\n",
      "18038 y\n",
      "15525 X\n",
      "15525 y\n",
      "10136 X\n",
      "10136 y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"    # Process the location data and get the final model\\n    final_model = process_location(X_train, y_train, loc)\\n    # Predict on new data\\n    predictions = predict_model(final_model, data=X_test)\\n        #print(predictions.columns)\\n        # Assuming 'predictions' is the output from predict_model\\n    final_predictions = predictions['prediction_label']\\n    print(final_predictions.dtypes)\\n    print(is_day_feature.dtypes)\\n    # Multiply final predictions with the 'is_day:idx' values\\n    adjusted_final_predictions = final_predictions * is_day_feature['is_day:idx']\\n\\n    # Now, 'adjusted_final_predictions' contains the adjusted target values.\\n\\n        # Store predictions\\n    all_predictions.append(adjusted_final_predictions)  \\n\\n        # Prepare dataframe for visualization\\n    num_predictions = len(predictions)\\n    timestamps = pd.date_range(start='2023-05-01 00:00:00', periods=num_predictions, freq='H')\\n\\n    df = pd.DataFrame({\\n            'time': timestamps,\\n            'prediction': predictions['prediction_label'],\\n            'location': loc\\n        })\\n\\n    final_df_list.append(df)\\n\\n\\n# Concatenate all the individual data frames and prepare for plotting\\nfinal_df = pd.concat(final_df_list, ignore_index=True)\\nfinal_df['time'] = pd.to_datetime(final_df['time'])\\nfinal_df.sort_values('time', inplace=True)\\n\\n# Visualization\\nfor loc in locations:\\n    # Filter data for each location\\n    temp_df = final_df[final_df['location'] == loc]\\n    \\n    plt.figure(figsize=(12, 6))  # Create a new figure for each location\\n    plt.plot(temp_df['time'], temp_df['prediction'], label=f'Location {loc}')\\n    plt.xlabel('Time')\\n    plt.ylabel('Predictions')\\n    plt.title(f'Predictions Over Time for Location {loc}')\\n    plt.legend()\\n    plt.show()\\n\\n# If you need to save the final dataframe without 'time' and 'location' columns, you can do so before plotting\\nfinal_df_save = final_df.copy()\\nfinal_df_save = final_df_save.drop(columns=['time', 'location'])\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = ['A', 'B', 'C']\n",
    "all_predictions = []\n",
    "final_df_list = [] \n",
    "\n",
    "\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    '''train_non_zero = train[train['pv_measurement'] != 0]\n",
    "    if train_non_zero.empty:\n",
    "        print(f\"train_non_zero is empty for location {loc}\")\n",
    "\n",
    "    # Get the timestamps of these rows\n",
    "    valid_timestamps = train_non_zero['time'].tolist()\n",
    "    \n",
    "    # Filter the X_* dataframes based on these timestamps\n",
    "    X_train_estimated = X_train_estimated[X_train_estimated['date_forecast'].isin(valid_timestamps)]\n",
    "    X_train_observed = X_train_observed[X_train_observed['date_forecast'].isin(valid_timestamps)]\n",
    "    #X_test_estimated = X_test_estimated[X_test_estimated['date_forecast'].isin(valid_timestamps)]\n",
    "    if X_train_estimated.empty:\n",
    "        print(f\"X_train_estimated is empty for location {loc}\")'''\n",
    "\n",
    "    #lage dag og måned feature der man aggregerer opp \n",
    "    # Preprocess data\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    y_train = targets\n",
    "    if X_train.empty or len(y_train) == 0:\n",
    "        print(f\"X_train or y_train is empty for location {loc}\")\n",
    "\n",
    "    # Ensure X and y have the same length\n",
    "    print(len(X_train), 'X')\n",
    "    print(len(y_train),'y')\n",
    "    min_length = min(len(X_train), len(y_train))\n",
    "    X_train, y_train = X_train.iloc[:min_length], y_train[:min_length]\n",
    "    \n",
    "'''    # Process the location data and get the final model\n",
    "    final_model = process_location(X_train, y_train, loc)\n",
    "    # Predict on new data\n",
    "    predictions = predict_model(final_model, data=X_test)\n",
    "        #print(predictions.columns)\n",
    "        # Assuming 'predictions' is the output from predict_model\n",
    "    final_predictions = predictions['prediction_label']\n",
    "    print(final_predictions.dtypes)\n",
    "    print(is_day_feature.dtypes)\n",
    "    # Multiply final predictions with the 'is_day:idx' values\n",
    "    adjusted_final_predictions = final_predictions * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Now, 'adjusted_final_predictions' contains the adjusted target values.\n",
    "\n",
    "        # Store predictions\n",
    "    all_predictions.append(adjusted_final_predictions)  \n",
    "\n",
    "        # Prepare dataframe for visualization\n",
    "    num_predictions = len(predictions)\n",
    "    timestamps = pd.date_range(start='2023-05-01 00:00:00', periods=num_predictions, freq='H')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "            'time': timestamps,\n",
    "            'prediction': predictions['prediction_label'],\n",
    "            'location': loc\n",
    "        })\n",
    "\n",
    "    final_df_list.append(df)\n",
    "\n",
    "\n",
    "# Concatenate all the individual data frames and prepare for plotting\n",
    "final_df = pd.concat(final_df_list, ignore_index=True)\n",
    "final_df['time'] = pd.to_datetime(final_df['time'])\n",
    "final_df.sort_values('time', inplace=True)\n",
    "\n",
    "# Visualization\n",
    "for loc in locations:\n",
    "    # Filter data for each location\n",
    "    temp_df = final_df[final_df['location'] == loc]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Create a new figure for each location\n",
    "    plt.plot(temp_df['time'], temp_df['prediction'], label=f'Location {loc}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'Predictions Over Time for Location {loc}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# If you need to save the final dataframe without 'time' and 'location' columns, you can do so before plotting\n",
    "final_df_save = final_df.copy()\n",
    "final_df_save = final_df_save.drop(columns=['time', 'location'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thoma\\OneDrive\\Documents\\GitHub\\tdt4173-machine-learning-project\\SolarEnergyProductionEnergy\\data\\pycaret.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''sample_submission = pd.read_csv('sample_submission.csv')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39msample_submission\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39msample_submission.to_csv('my_first_submission.csv', index=False)'''\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m final_predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(all_predictions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Save the final_predictions to CSV\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/OneDrive/Documents/GitHub/tdt4173-machine-learning-project/SolarEnergyProductionEnergy/data/pycaret.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(final_predictions, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "'''sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)'''\n",
    "\n",
    "final_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('final_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
