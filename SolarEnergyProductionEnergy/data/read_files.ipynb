{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from catboost import CatBoostRegressor\n",
    "#from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: .\n",
      "  File: my_first_submission.csv\n",
      "  File: .DS_Store\n",
      "  File: test.csv\n",
      "  File: Readme.md\n",
      "  File: catBoost.ipynb\n",
      "  File: sample_submission.csv\n",
      "  File: read_files.ipynb\n",
      "Directory: ./A\n",
      "  File: X_train_observed.parquet\n",
      "  File: train_targets.parquet\n",
      "  File: X_train_estimated.parquet\n",
      "  File: X_test_estimated.parquet\n",
      "Directory: ./C\n",
      "  File: X_train_observed.parquet\n",
      "  File: train_targets.parquet\n",
      "  File: X_train_estimated.parquet\n",
      "  File: X_test_estimated.parquet\n",
      "Directory: ./B\n",
      "  File: X_train_observed.parquet\n",
      "  File: train_targets.parquet\n",
      "  File: X_train_estimated.parquet\n",
      "  File: X_test_estimated.parquet\n",
      "Directory: ./catboost_info\n",
      "  File: learn_error.tsv\n",
      "  File: .DS_Store\n",
      "  File: test_error.tsv\n",
      "  File: time_left.tsv\n",
      "  File: catboost_training.json\n",
      "Directory: ./catboost_info/learn\n",
      "  File: events.out.tfevents\n",
      "Directory: ./catboost_info/test\n",
      "  File: events.out.tfevents\n",
      "Directory: ./catboost_info/tmp\n",
      "  File: cat_feature_index.eaa4b21a-9ccafede-c0d84f1d-2cb9343f.tmp\n",
      "  File: cat_feature_index.454572d9-adb4676c-562ed730-fcaa1942.tmp\n",
      "  File: cat_feature_index.c8a496a9-1d28a500-7800fdc9-da23a73a.tmp\n",
      "  File: cat_feature_index.275b4d3c-85b73e49-69c60451-ec9fcb4b.tmp\n",
      "  File: cat_feature_index.325b98ae-2478473b-292d118-d2b0e53a.tmp\n",
      "  File: cat_feature_index.f9426096-693d3297-b763d195-3c3cc42d.tmp\n",
      "  File: cat_feature_index.465a3a49-6e85797f-191e99c8-59b01f09.tmp\n",
      "  File: cat_feature_index.183104b5-642a4438-9869cb06-fbb87aec.tmp\n",
      "  File: cat_feature_index.a29aa209-842528e8-222b91e-a9630411.tmp\n",
      "  File: cat_feature_index.9d046aef-b12346d8-bd105fd1-e7091f82.tmp\n",
      "  File: cat_feature_index.7540e618-9176bc27-fd8823dc-3a989de6.tmp\n",
      "  File: cat_feature_index.6bfa13da-a3c1dcc6-341432c2-783c84f1.tmp\n",
      "  File: cat_feature_index.bc232664-129b1e1e-498900cd-e8679836.tmp\n",
      "  File: cat_feature_index.6addfa42-c2fe2d96-58118249-f43964ab.tmp\n",
      "  File: cat_feature_index.8fd22e91-56250406-e225f282-92692974.tmp\n",
      "  File: cat_feature_index.ab6f8121-5d922f29-cd459c79-a14be223.tmp\n"
     ]
    }
   ],
   "source": [
    "def list_directory_tree_with_os_walk(starting_directory):\n",
    "    for root, directories, files in os.walk(starting_directory):\n",
    "        print(f\"Directory: {root}\")\n",
    "        for file in files:\n",
    "            print(f\"  File: {file}\")\n",
    "\n",
    "list_directory_tree_with_os_walk('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Set the index to the time column\\ntrain_a.set_index('time', inplace=True)\\ntrain_b.set_index('time', inplace=True)\\ntrain_c.set_index('time', inplace=True)\\n\\n# Replace zero values with NaN\\ntrain_a.replace(0, np.nan, inplace=True)\\ntrain_b.replace(0, np.nan, inplace=True)\\ntrain_c.replace(0, np.nan, inplace=True)\\n\\n# Perform time-based interpolation\\ntrain_a.interpolate(method='time', inplace=True)\\ntrain_b.interpolate(method='time', inplace=True)\\ntrain_c.interpolate(method='time', inplace=True)\\n\\n# Define the window size for the moving average\\nwindow_size = 3\\n\\n# Compute the moving average\\ntrain_a_smoothed = train_a.rolling(window=window_size).mean()\\ntrain_b_smoothed = train_b.rolling(window=window_size).mean()\\ntrain_c_smoothed = train_c.rolling(window=window_size).mean()\\n\\n# Assuming all datasets have the same features\\nfeatures = [col for col in train_a.columns]\\n\\nfor feature_name in features:\\n    fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\\n    \\n    # Plotting for dataset A\\n    train_a[feature_name].plot(ax=axs[0], title=f'Train A - {feature_name}')\\n    train_a_smoothed[feature_name].plot(ax=axs[0], linestyle='--', label=f'{window_size}-Day Moving Average')\\n    \\n    # Plotting for dataset B\\n    train_b[feature_name].plot(ax=axs[1], title=f'Train B - {feature_name}')\\n    train_b_smoothed[feature_name].plot(ax=axs[1], linestyle='--', label=f'{window_size}-Day Moving Average')\\n    \\n    # Plotting for dataset C\\n    train_c[feature_name].plot(ax=axs[2], title=f'Train C - {feature_name}')\\n    train_c_smoothed[feature_name].plot(ax=axs[2], linestyle='--', label=f'{window_size}-Day Moving Average')\\n    \\n    # Adding legends\\n    axs[0].legend()\\n    axs[1].legend()\\n    axs[2].legend()\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\n# Replace zero values with NaN\\ntrain_a_smoothed.replace(np.nan,0, inplace=True)\\ntrain_b_smoothed.replace(np.nan,0, inplace=True)\\ntrain_c_smoothed.replace(np.nan,0, inplace=True)\\n\\ntrain_a.replace( np.nan,0, inplace=True)\\ntrain_b.replace(np.nan,0, inplace=True)\\ntrain_c.replace( np.nan,0, inplace=True)\""
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "#interpolation and moving avarage, test this again with new model\n",
    "'''# Set the index to the time column\n",
    "train_a.set_index('time', inplace=True)\n",
    "train_b.set_index('time', inplace=True)\n",
    "train_c.set_index('time', inplace=True)\n",
    "\n",
    "# Replace zero values with NaN\n",
    "train_a.replace(0, np.nan, inplace=True)\n",
    "train_b.replace(0, np.nan, inplace=True)\n",
    "train_c.replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Perform time-based interpolation\n",
    "train_a.interpolate(method='time', inplace=True)\n",
    "train_b.interpolate(method='time', inplace=True)\n",
    "train_c.interpolate(method='time', inplace=True)\n",
    "\n",
    "# Define the window size for the moving average\n",
    "window_size = 3\n",
    "\n",
    "# Compute the moving average\n",
    "train_a_smoothed = train_a.rolling(window=window_size).mean()\n",
    "train_b_smoothed = train_b.rolling(window=window_size).mean()\n",
    "train_c_smoothed = train_c.rolling(window=window_size).mean()\n",
    "\n",
    "# Assuming all datasets have the same features\n",
    "features = [col for col in train_a.columns]\n",
    "\n",
    "for feature_name in features:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\n",
    "    \n",
    "    # Plotting for dataset A\n",
    "    train_a[feature_name].plot(ax=axs[0], title=f'Train A - {feature_name}')\n",
    "    train_a_smoothed[feature_name].plot(ax=axs[0], linestyle='--', label=f'{window_size}-Day Moving Average')\n",
    "    \n",
    "    # Plotting for dataset B\n",
    "    train_b[feature_name].plot(ax=axs[1], title=f'Train B - {feature_name}')\n",
    "    train_b_smoothed[feature_name].plot(ax=axs[1], linestyle='--', label=f'{window_size}-Day Moving Average')\n",
    "    \n",
    "    # Plotting for dataset C\n",
    "    train_c[feature_name].plot(ax=axs[2], title=f'Train C - {feature_name}')\n",
    "    train_c_smoothed[feature_name].plot(ax=axs[2], linestyle='--', label=f'{window_size}-Day Moving Average')\n",
    "    \n",
    "    # Adding legends\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    axs[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Replace zero values with NaN\n",
    "train_a_smoothed.replace(np.nan,0, inplace=True)\n",
    "train_b_smoothed.replace(np.nan,0, inplace=True)\n",
    "train_c_smoothed.replace(np.nan,0, inplace=True)\n",
    "\n",
    "train_a.replace( np.nan,0, inplace=True)\n",
    "train_b.replace(np.nan,0, inplace=True)\n",
    "train_c.replace( np.nan,0, inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_calc                           184\n",
      "date_forecast                     17576\n",
      "absolute_humidity_2m:gm3             79\n",
      "air_density_2m:kgm3                 162\n",
      "ceiling_height_agl:m              10832\n",
      "clear_sky_energy_1h:J              6863\n",
      "clear_sky_rad:W                    3270\n",
      "cloud_base_agl:m                  11280\n",
      "dew_or_rime:idx                       3\n",
      "dew_point_2m:K                      275\n",
      "diffuse_rad:W                      1798\n",
      "diffuse_rad_1h:J                   6868\n",
      "direct_rad:W                       1968\n",
      "direct_rad_1h:J                    5543\n",
      "effective_cloud_cover:p            1001\n",
      "elevation:m                           1\n",
      "fresh_snow_12h:cm                    84\n",
      "fresh_snow_1h:cm                     22\n",
      "fresh_snow_24h:cm                   127\n",
      "fresh_snow_3h:cm                     41\n",
      "fresh_snow_6h:cm                     59\n",
      "is_day:idx                            2\n",
      "is_in_shadow:idx                      2\n",
      "msl_pressure:hPa                    704\n",
      "precip_5min:mm                       23\n",
      "precip_type_5min:idx                  4\n",
      "pressure_100m:hPa                   696\n",
      "pressure_50m:hPa                    699\n",
      "prob_rime:p                         296\n",
      "rain_water:kgm2                       5\n",
      "relative_humidity_1000hPa:p         751\n",
      "sfc_pressure:hPa                    702\n",
      "snow_density:kgm3                     1\n",
      "snow_depth:cm                        50\n",
      "snow_drift:idx                        1\n",
      "snow_melt_10min:mm                   15\n",
      "snow_water:kgm2                      25\n",
      "sun_azimuth:d                     17107\n",
      "sun_elevation:d                   15700\n",
      "super_cooled_liquid_water:kgm2       10\n",
      "t_1000hPa:K                         263\n",
      "total_cloud_cover:p                1001\n",
      "visibility:m                      17309\n",
      "wind_speed_10m:ms                   103\n",
      "wind_speed_u_10m:ms                 174\n",
      "wind_speed_v_10m:ms                 122\n",
      "wind_speed_w_1000hPa:ms               1\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'okey, can you make a code in python that test Feature Transformation: Try transforming these features (e.g., logarithmic, polynomial transformations) to see if they can be made more predictive.\\nFeature Interaction: Create interaction terms between features. Sometimes, the combination of two features can be more predictive than each feature on its own. on our data, do some diffrent transformations and try to feature interact all possibilities and give me back the best feature eng'"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Værmelding, ulike parameter fra meteomatics\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "print(X_train_estimated_a.nunique())\n",
    "'''okey, can you make a code in python that test Feature Transformation: Try transforming these features (e.g., logarithmic, polynomial transformations) to see if they can be made more predictive.\n",
    "Feature Interaction: Create interaction terms between features. Sometimes, the combination of two features can be more predictive than each feature on its own. on our data, do some diffrent transformations and try to feature interact all possibilities and give me back the best feature eng'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observert vær\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Værmelding. Må matche med X_test_observed. Men dette er frem i tid. \n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging estimated and observed data with the power output, but we might need to investigate if there is some differece in observed weather and weather forcast\n",
    "# Combine estimated and observed data for location A\n",
    "X_train_a = pd.concat([X_train_estimated_a, X_train_observed_a])\n",
    "\n",
    "# Merge features and target data for location A\n",
    "merged_data_a = pd.merge(X_train_a, train_a, left_on='date_forecast', right_on='time')\n",
    "\n",
    "\n",
    "# Combine estimated and observed data for location A\n",
    "X_train_b = pd.concat([X_train_estimated_b, X_train_observed_b])\n",
    "\n",
    "# Merge features and target data for location A\n",
    "merged_data_b = pd.merge(X_train_b, train_b, left_on='date_forecast', right_on='time')\n",
    "\n",
    "# Now, calculate the correlation matrix\n",
    "correlation_matrix_b = merged_data_b.corr()\n",
    "\n",
    "# You might want to focus on the correlations with the target variable\n",
    "correlations_with_target_b = correlation_matrix_b['pv_measurement'].sort_values(ascending=False)\n",
    "#print(correlations_with_target_b)\n",
    "\n",
    "# Combine estimated and observed data for location A\n",
    "X_train_c = pd.concat([X_train_estimated_c, X_train_observed_c])\n",
    "\n",
    "# Merge features and target data for location A\n",
    "merged_data_c = pd.merge(X_train_c, train_c, left_on='date_forecast', right_on='time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Feature  Location_A  Location_B  Location_C  \\\n",
      "12                    visibility:m    0.228371    0.121831    0.151019   \n",
      "23                     prob_rime:p   -0.041333   -0.021046   -0.113571   \n",
      "9                      t_1000hPa:K    0.335902    0.416585    0.427087   \n",
      "11        absolute_humidity_2m:gm3    0.236673    0.323513    0.319422   \n",
      "10                  dew_point_2m:K    0.245950    0.323798    0.325013   \n",
      "26                 dew_or_rime:idx   -0.057400   -0.024646    0.018164   \n",
      "45                is_in_shadow:idx   -0.576389   -0.510066   -0.584903   \n",
      "22                            time   -0.033503   -0.107539   -0.052976   \n",
      "21                   date_forecast   -0.033503   -0.107539   -0.052976   \n",
      "40             total_cloud_cover:p   -0.187265   -0.142365   -0.117943   \n",
      "41         effective_cloud_cover:p   -0.216672   -0.171663   -0.147640   \n",
      "43             air_density_2m:kgm3   -0.356402   -0.367909   -0.421306   \n",
      "4            clear_sky_energy_1h:J    0.781216    0.778166    0.842577   \n",
      "8                       is_day:idx    0.539552    0.475470    0.503667   \n",
      "2                  direct_rad_1h:J    0.830861    0.787618    0.849950   \n",
      "18                cloud_base_agl:m    0.157650    0.191939    0.131968   \n",
      "3                  clear_sky_rad:W    0.803538    0.797163    0.855853   \n",
      "42             wind_speed_v_10m:ms   -0.260477   -0.253483   -0.307637   \n",
      "1                     direct_rad:W    0.856095    0.807989    0.860943   \n",
      "6                 diffuse_rad_1h:J    0.687206    0.672463    0.718173   \n",
      "5                    diffuse_rad:W    0.702831    0.683735    0.726965   \n",
      "33                 rain_water:kgm2   -0.072587   -0.032433   -0.047836   \n",
      "38  super_cooled_liquid_water:kgm2   -0.110842   -0.083701   -0.071926   \n",
      "44     relative_humidity_1000hPa:p   -0.365291   -0.327933   -0.355607   \n",
      "7                  sun_elevation:d    0.683479    0.652883    0.678303   \n",
      "30                   sun_azimuth:d   -0.062588   -0.051541   -0.032489   \n",
      "29             wind_speed_u_10m:ms   -0.059352   -0.047119   -0.032021   \n",
      "20              snow_melt_10min:mm   -0.011962   -0.022971   -0.038088   \n",
      "32                   snow_depth:cm   -0.065209   -0.083834   -0.087439   \n",
      "19         wind_speed_w_1000hPa:ms   -0.006044   -0.006153    0.013215   \n",
      "39                 snow_water:kgm2   -0.124654   -0.105356   -0.108049   \n",
      "17            ceiling_height_agl:m    0.162789    0.156788    0.175998   \n",
      "37                       date_calc   -0.096108   -0.083572   -0.077733   \n",
      "36               wind_speed_10m:ms   -0.089403   -0.094233   -0.076326   \n",
      "27                  precip_5min:mm   -0.057728   -0.042473   -0.049397   \n",
      "35            precip_type_5min:idx   -0.081785   -0.069048   -0.072777   \n",
      "13               pressure_100m:hPa    0.183608    0.192812    0.190648   \n",
      "15                pressure_50m:hPa    0.178821    0.187423    0.184843   \n",
      "16                sfc_pressure:hPa    0.174001    0.182072    0.179021   \n",
      "31               fresh_snow_12h:cm   -0.064208   -0.070343   -0.066914   \n",
      "34               fresh_snow_24h:cm   -0.080449   -0.084267   -0.081665   \n",
      "14                msl_pressure:hPa    0.183326    0.183671    0.186977   \n",
      "28                fresh_snow_6h:cm   -0.058336   -0.060326   -0.060953   \n",
      "24                fresh_snow_1h:cm   -0.042882   -0.042724   -0.044786   \n",
      "25                fresh_snow_3h:cm   -0.051812   -0.052677   -0.052956   \n",
      "0                   pv_measurement    1.000000    1.000000    1.000000   \n",
      "46                     elevation:m         NaN         NaN         NaN   \n",
      "47               snow_density:kgm3         NaN         NaN         NaN   \n",
      "48                  snow_drift:idx         NaN   -0.003899         NaN   \n",
      "\n",
      "    Max_Diff  \n",
      "12  0.106540  \n",
      "23  0.092525  \n",
      "9   0.091186  \n",
      "11  0.086840  \n",
      "10  0.079064  \n",
      "26  0.075564  \n",
      "45  0.074836  \n",
      "22  0.074036  \n",
      "21  0.074036  \n",
      "40  0.069323  \n",
      "41  0.069032  \n",
      "43  0.064904  \n",
      "4   0.064411  \n",
      "8   0.064081  \n",
      "2   0.062332  \n",
      "18  0.059971  \n",
      "3   0.058690  \n",
      "42  0.054154  \n",
      "1   0.052954  \n",
      "6   0.045710  \n",
      "5   0.043230  \n",
      "33  0.040153  \n",
      "38  0.038917  \n",
      "44  0.037359  \n",
      "7   0.030596  \n",
      "30  0.030099  \n",
      "29  0.027331  \n",
      "20  0.026125  \n",
      "32  0.022229  \n",
      "19  0.019368  \n",
      "39  0.019299  \n",
      "17  0.019210  \n",
      "37  0.018376  \n",
      "36  0.017907  \n",
      "27  0.015254  \n",
      "35  0.012737  \n",
      "13  0.009204  \n",
      "15  0.008602  \n",
      "16  0.008071  \n",
      "31  0.006135  \n",
      "34  0.003817  \n",
      "14  0.003652  \n",
      "28  0.002618  \n",
      "24  0.002062  \n",
      "25  0.001143  \n",
      "0   0.000000  \n",
      "46       NaN  \n",
      "47       NaN  \n",
      "48       NaN  \n"
     ]
    }
   ],
   "source": [
    "#Here we are calculating the lineral correlation of each feature to the power output\n",
    "# Now, calculate the correlation matrix\n",
    "correlation_matrix_a = merged_data_a.corr()\n",
    "\n",
    "# You might want to focus on the correlations with the target variable\n",
    "correlations_with_target_a = correlation_matrix_a['pv_measurement'].sort_values(ascending=False)\n",
    "#print(correlations_with_target_a)\n",
    "\n",
    "# Now, calculate the correlation matrix\n",
    "correlation_matrix_b = merged_data_b.corr()\n",
    "\n",
    "# You might want to focus on the correlations with the target variable\n",
    "correlations_with_target_b = correlation_matrix_b['pv_measurement'].sort_values(ascending=False)\n",
    "#print(correlations_with_target_b)\n",
    "\n",
    "# Now, calculate the correlation matrix\n",
    "correlation_matrix_c = merged_data_c.corr()\n",
    "\n",
    "# You might want to focus on the correlations with the target variable\n",
    "correlations_with_target_c = correlation_matrix_c['pv_measurement'].sort_values(ascending=False)\n",
    "#print(correlations_with_target_c)\n",
    "\n",
    "\n",
    "# Create a DataFrame to hold the correlation values\n",
    "correlation_comparison = pd.DataFrame({\n",
    "    'Feature': correlations_with_target_a.index,\n",
    "    'Location_A': correlations_with_target_a.values,\n",
    "    'Location_B': correlations_with_target_b.reindex(correlations_with_target_a.index).values,\n",
    "    'Location_C': correlations_with_target_c.reindex(correlations_with_target_a.index).values\n",
    "})\n",
    "\n",
    "# Calculate the absolute difference in correlation values across locations\n",
    "correlation_comparison['Max_Diff'] = correlation_comparison.apply(\n",
    "    lambda row: max(row['Location_A'], row['Location_B'], row['Location_C']) - \n",
    "                min(row['Location_A'], row['Location_B'], row['Location_C']), axis=1)\n",
    "\n",
    "\n",
    "# Sort the DataFrame based on the difference in correlation values\n",
    "correlation_comparison = correlation_comparison.sort_values(by='Max_Diff', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(correlation_comparison)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.DataFrame(correlation_comparison)\\n\\n# Calculate the mean of the absolute values across the three locations\\ndf['mean_abs_value'] = df[['Location_A', 'Location_B', 'Location_C']].abs().mean(axis=1)\\n\\n# Sort the dataframe by the mean_abs_value in descending order\\nsorted_df = df.sort_values(by='mean_abs_value', ascending=False)\\n\\n# Extract the feature names from the sorted dataframe\\nselected_features = sorted_df['Feature'].tolist()\\n\\nprint(selected_features)\""
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selecting features with the highest absolute values\n",
    "\n",
    "'''df = pd.DataFrame(correlation_comparison)\n",
    "\n",
    "# Calculate the mean of the absolute values across the three locations\n",
    "df['mean_abs_value'] = df[['Location_A', 'Location_B', 'Location_C']].abs().mean(axis=1)\n",
    "\n",
    "# Sort the dataframe by the mean_abs_value in descending order\n",
    "sorted_df = df.sort_values(by='mean_abs_value', ascending=False)\n",
    "\n",
    "# Extract the feature names from the sorted dataframe\n",
    "selected_features = sorted_df['Feature'].tolist()\n",
    "\n",
    "print(selected_features)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\\nfeature_name = 'absolute_humidity_2m:gm3'\\nX_train_observed_a[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title='Train/Test A', color='red')\\nX_train_estimated_a[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title='Train/Test A', color='blue')\\nX_test_estimated_a[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title='Train/Test  A', color='green')\\n\\nX_train_observed_b[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title='Train/Test  B', color='red')\\nX_train_estimated_b[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title='Train/Test  B', color='blue')\\nX_test_estimated_b[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title='Train/Test  B', color='green')\\n\\nX_train_observed_c[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[2], title='Train/Test  C', color='red')\\nX_train_estimated_c[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[2], title='Train/Test  C', color='blue')\\nX_test_estimated_c[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[2], title='Train/Test  C', color='green')\""
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting a single feature\n",
    "'''fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\n",
    "feature_name = 'absolute_humidity_2m:gm3'\n",
    "X_train_observed_a[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title='Train/Test A', color='red')\n",
    "X_train_estimated_a[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title='Train/Test A', color='blue')\n",
    "X_test_estimated_a[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[0], title='Train/Test  A', color='green')\n",
    "\n",
    "X_train_observed_b[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title='Train/Test  B', color='red')\n",
    "X_train_estimated_b[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title='Train/Test  B', color='blue')\n",
    "X_test_estimated_b[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[1], title='Train/Test  B', color='green')\n",
    "\n",
    "X_train_observed_c[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[2], title='Train/Test  C', color='red')\n",
    "X_train_estimated_c[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[2], title='Train/Test  C', color='blue')\n",
    "X_test_estimated_c[['date_forecast', feature_name]].set_index('date_forecast').plot(ax=axs[2], title='Train/Test  C', color='green')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>prediction</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2155</td>\n",
       "      <td>2023-07-03 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2156</td>\n",
       "      <td>2023-07-03 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2157</td>\n",
       "      <td>2023-07-03 21:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2158</td>\n",
       "      <td>2023-07-03 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2159</td>\n",
       "      <td>2023-07-03 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                 time  prediction location\n",
       "0        0  2023-05-01 00:00:00           0        A\n",
       "1        1  2023-05-01 01:00:00           0        A\n",
       "2        2  2023-05-01 02:00:00           0        A\n",
       "3        3  2023-05-01 03:00:00           0        A\n",
       "4        4  2023-05-01 04:00:00           0        A\n",
       "...    ...                  ...         ...      ...\n",
       "2155  2155  2023-07-03 19:00:00           0        C\n",
       "2156  2156  2023-07-03 20:00:00           0        C\n",
       "2157  2157  2023-07-03 21:00:00           0        C\n",
       "2158  2158  2023-07-03 22:00:00           0        C\n",
       "2159  2159  2023-07-03 23:00:00           0        C\n",
       "\n",
       "[2160 rows x 4 columns]"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0        0           0\n",
       "1        1           0\n",
       "2        2           0\n",
       "3        3           0\n",
       "4        4           0\n",
       "...    ...         ...\n",
       "2155  2155           0\n",
       "2156  2156           0\n",
       "2157  2157           0\n",
       "2158  2158           0\n",
       "2159  2159           0\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                 time  prediction location\n",
      "0        0  2023-05-01 00:00:00    0.420373        A\n",
      "1        1  2023-05-01 01:00:00    0.572254        A\n",
      "2        2  2023-05-01 02:00:00    0.965502        A\n",
      "3        3  2023-05-01 03:00:00    0.596035        A\n",
      "4        4  2023-05-01 04:00:00    0.443984        A\n",
      "...    ...                  ...         ...      ...\n",
      "2155  2155  2023-07-03 19:00:00    0.479380        C\n",
      "2156  2156  2023-07-03 20:00:00    0.010849        C\n",
      "2157  2157  2023-07-03 21:00:00    0.868973        C\n",
      "2158  2158  2023-07-03 22:00:00    0.944916        C\n",
      "2159  2159  2023-07-03 23:00:00    0.224678        C\n",
      "\n",
      "[2160 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example, let the predictions be random values\n",
    "test['prediction'] = np.random.rand(len(test))\n",
    "print(test)\n",
    "sample_submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error \n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): n-dimensional vector of ground-truth values \n",
    "        y_pred (np.array): n-dimensional vecotr of predicted values \n",
    "    \n",
    "    Returns:\n",
    "        A scalar float with the rmsle value \n",
    "    \n",
    "    Note: You can alternatively use sklearn and just do: \n",
    "        sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    \"\"\"\n",
    "    assert (y_true >= 0).all(), 'Received negative y_true values'\n",
    "    assert (y_pred >= 0).all(), 'Received negative y_pred values'\n",
    "    assert y_true.shape == y_pred.shape, 'y_true and y_pred have different shapes'\n",
    "    y_true_log1p = np.log1p(y_true)  # log(1 + y_true)\n",
    "    y_pred_log1p = np.log1p(y_pred)  # log(1 + y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Managing the data\\n\\nX_train_a = pd.concat([X_train_observed_a, X_train_estimated_a])\\nX_train_b = pd.concat([X_train_observed_b, X_train_estimated_b])\\nX_train_c = pd.concat([X_train_observed_c, X_train_estimated_c])\\n\\n# Split the merged data into features (X) and target (y)\\nmerged_data = pd.merge(X_train_a, train_a, left_on='date_forecast', right_on='time')\\nmerged_data_b = pd.merge(X_train_b, train_b, left_on='date_forecast', right_on='time')\\nmerged_data_c = pd.merge(X_train_c, train_c, left_on='date_forecast', right_on='time')\\n\\n#Adding hour, day month and year as parameters in the model from dataforecast\\nmerged_data['hour'] = merged_data['date_forecast'].dt.hour\\nmerged_data['day'] = merged_data['date_forecast'].dt.day\\nmerged_data['month'] = merged_data['date_forecast'].dt.month\\nmerged_data['year'] = merged_data['date_forecast'].dt.year\\n\\nmerged_data_b['hour'] = merged_data_b['date_forecast'].dt.hour\\nmerged_data_b['day'] = merged_data_b['date_forecast'].dt.day\\nmerged_data_b['month'] = merged_data_b['date_forecast'].dt.month\\nmerged_data_b['year'] = merged_data_b['date_forecast'].dt.year\\n\\nmerged_data_c['hour'] = merged_data_c['date_forecast'].dt.hour\\nmerged_data_c['day'] = merged_data_c['date_forecast'].dt.day\\nmerged_data_c['month'] = merged_data_c['date_forecast'].dt.month\\nmerged_data_c['year'] = merged_data_c['date_forecast'].dt.year\\n\\n#Splitting into X and Y values\\ny = merged_data['pv_measurement']\\nX = merged_data.drop(columns=['pv_measurement', 'date_forecast', 'date_calc', 'time'])\\n\\ny_b = merged_data_b['pv_measurement']\\nX_b = merged_data_b.drop(columns=['pv_measurement', 'date_forecast', 'date_calc','time'])\\n\\ny_c = merged_data_c['pv_measurement']\\nX_c = merged_data_c.drop(columns=['pv_measurement', 'date_forecast', 'date_calc','time'])\\n\\n#Same but for the test estimated set\\nX_test_estimated_a['hour'] = X_test_estimated_a['date_forecast'].dt.hour\\nX_test_estimated_a['day'] = X_test_estimated_a['date_forecast'].dt.day\\nX_test_estimated_a['month'] = X_test_estimated_a['date_forecast'].dt.month\\nX_test_estimated_a['year'] = X_test_estimated_a['date_forecast'].dt.year\\n\\nX_test_estimated_b['hour'] = X_test_estimated_b['date_forecast'].dt.hour\\nX_test_estimated_b['day'] = X_test_estimated_b['date_forecast'].dt.day\\nX_test_estimated_b['month'] = X_test_estimated_b['date_forecast'].dt.month\\nX_test_estimated_b['year'] = X_test_estimated_b['date_forecast'].dt.year\\n\\nX_test_estimated_c['hour'] = X_test_estimated_c['date_forecast'].dt.hour\\nX_test_estimated_c['day'] = X_test_estimated_c['date_forecast'].dt.day\\nX_test_estimated_c['month'] = X_test_estimated_c['date_forecast'].dt.month\\nX_test_estimated_c['year'] = X_test_estimated_c['date_forecast'].dt.year\\n\\n#Dropping dates\\nX_test_a = X_test_estimated_a.drop(columns=[ 'date_calc', 'date_forecast'])\\nX_test_b = X_test_estimated_b.drop(columns=[ 'date_calc', 'date_forecast']) \\nX_test_c = X_test_estimated_c.drop(columns=[  'date_calc', 'date_forecast'])\""
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Managing the data\n",
    "\n",
    "X_train_a = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "X_train_b = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "X_train_c = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "# Split the merged data into features (X) and target (y)\n",
    "merged_data = pd.merge(X_train_a, train_a, left_on='date_forecast', right_on='time')\n",
    "merged_data_b = pd.merge(X_train_b, train_b, left_on='date_forecast', right_on='time')\n",
    "merged_data_c = pd.merge(X_train_c, train_c, left_on='date_forecast', right_on='time')\n",
    "\n",
    "#Adding hour, day month and year as parameters in the model from dataforecast\n",
    "merged_data['hour'] = merged_data['date_forecast'].dt.hour\n",
    "merged_data['day'] = merged_data['date_forecast'].dt.day\n",
    "merged_data['month'] = merged_data['date_forecast'].dt.month\n",
    "merged_data['year'] = merged_data['date_forecast'].dt.year\n",
    "\n",
    "merged_data_b['hour'] = merged_data_b['date_forecast'].dt.hour\n",
    "merged_data_b['day'] = merged_data_b['date_forecast'].dt.day\n",
    "merged_data_b['month'] = merged_data_b['date_forecast'].dt.month\n",
    "merged_data_b['year'] = merged_data_b['date_forecast'].dt.year\n",
    "\n",
    "merged_data_c['hour'] = merged_data_c['date_forecast'].dt.hour\n",
    "merged_data_c['day'] = merged_data_c['date_forecast'].dt.day\n",
    "merged_data_c['month'] = merged_data_c['date_forecast'].dt.month\n",
    "merged_data_c['year'] = merged_data_c['date_forecast'].dt.year\n",
    "\n",
    "#Splitting into X and Y values\n",
    "y = merged_data['pv_measurement']\n",
    "X = merged_data.drop(columns=['pv_measurement', 'date_forecast', 'date_calc', 'time'])\n",
    "\n",
    "y_b = merged_data_b['pv_measurement']\n",
    "X_b = merged_data_b.drop(columns=['pv_measurement', 'date_forecast', 'date_calc','time'])\n",
    "\n",
    "y_c = merged_data_c['pv_measurement']\n",
    "X_c = merged_data_c.drop(columns=['pv_measurement', 'date_forecast', 'date_calc','time'])\n",
    "\n",
    "#Same but for the test estimated set\n",
    "X_test_estimated_a['hour'] = X_test_estimated_a['date_forecast'].dt.hour\n",
    "X_test_estimated_a['day'] = X_test_estimated_a['date_forecast'].dt.day\n",
    "X_test_estimated_a['month'] = X_test_estimated_a['date_forecast'].dt.month\n",
    "X_test_estimated_a['year'] = X_test_estimated_a['date_forecast'].dt.year\n",
    "\n",
    "X_test_estimated_b['hour'] = X_test_estimated_b['date_forecast'].dt.hour\n",
    "X_test_estimated_b['day'] = X_test_estimated_b['date_forecast'].dt.day\n",
    "X_test_estimated_b['month'] = X_test_estimated_b['date_forecast'].dt.month\n",
    "X_test_estimated_b['year'] = X_test_estimated_b['date_forecast'].dt.year\n",
    "\n",
    "X_test_estimated_c['hour'] = X_test_estimated_c['date_forecast'].dt.hour\n",
    "X_test_estimated_c['day'] = X_test_estimated_c['date_forecast'].dt.day\n",
    "X_test_estimated_c['month'] = X_test_estimated_c['date_forecast'].dt.month\n",
    "X_test_estimated_c['year'] = X_test_estimated_c['date_forecast'].dt.year\n",
    "\n",
    "#Dropping dates\n",
    "X_test_a = X_test_estimated_a.drop(columns=[ 'date_calc', 'date_forecast'])\n",
    "X_test_b = X_test_estimated_b.drop(columns=[ 'date_calc', 'date_forecast']) \n",
    "X_test_c = X_test_estimated_c.drop(columns=[  'date_calc', 'date_forecast'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX, y = process_dataset(X, y, 'A', features_to_convert)\\nX_b, y_b = process_dataset(X_b, y_b, 'B', features_to_convert)\\nX_c, y_c = process_dataset(X_c, y_c, 'C', features_to_convert)\""
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Improved manageing the train data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Preprocessing: Merging, feature extraction and splitting\n",
    "def preprocess_data(X_train_observed, X_train_estimated, train):\n",
    "    # Merging observed and estimated\n",
    "    X_train = pd.concat([X_train_observed, X_train_estimated])\n",
    "    \n",
    "    # Merging with train data\n",
    "    merged_data = pd.merge(X_train, train, left_on='date_forecast', right_on='time')\n",
    "    columns_to_extract = ['date_calc', 'date_forecast']\n",
    "    for column_name in columns_to_extract:\n",
    "        merged_data[f'{column_name}_hour'] = merged_data[column_name].dt.hour\n",
    "        merged_data[f'{column_name}_day'] = merged_data[column_name].dt.day\n",
    "        merged_data[f'{column_name}_month'] = merged_data[column_name].dt.month\n",
    "        merged_data[f'{column_name}_year'] = merged_data[column_name].dt.year\n",
    "        merged_data = merged_data.drop(columns=[column_name])\n",
    "    # Extracting date-related features\n",
    "    \n",
    "    # Splitting into X and y\n",
    "    y = merged_data['pv_measurement']\n",
    "    X = merged_data.drop(columns=['pv_measurement', 'time'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 2. Data processing: NaN replacement, feature conversion and adding location column\n",
    "def process_dataset(X_data, y_data, location_name, features_to_convert):\n",
    "    # Replace NaN values with 0\n",
    "    X_data.replace(np.nan, 0, inplace=True)\n",
    "    y_data.replace(np.nan, 0, inplace=True)\n",
    "    \n",
    "    # Add location column\n",
    "    X_data['location'] = location_name\n",
    "    \n",
    "    # Convert specified features to int type\n",
    "    for feature in features_to_convert:\n",
    "        X_data[feature] = X_data[feature].astype(int)\n",
    "    \n",
    "    return X_data, y_data\n",
    "\n",
    "# 3. Encode cyclical features: Du tenker sikkert \"hvorfor cyclical features?\". Fordi chattern sa dette: Time cyclicality should also be addressed. Time features like hour, day, and month are cyclic in nature. Consider converting them into cyclic features\n",
    "def encode_cyclical(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n",
    "    data = data.drop(col, axis=1)\n",
    "    return data\n",
    "\n",
    "# Apply the functions\n",
    "X, y = preprocess_data(X_train_observed_a, X_train_estimated_a, train_a)\n",
    "X_b, y_b = preprocess_data(X_train_observed_b, X_train_estimated_b, train_b)\n",
    "X_c, y_c = preprocess_data(X_train_observed_c, X_train_estimated_c, train_c)\n",
    "\n",
    "features_to_convert = ['is_day:idx','is_in_shadow:idx']\n",
    "\n",
    "\n",
    "X = encode_cyclical(X, 'date_forecast_hour', 24)\n",
    "X = encode_cyclical(X, 'date_forecast_day', 30.5)  # Average days in a month\n",
    "X = encode_cyclical(X, 'date_forecast_month', 12)\n",
    "\n",
    "X_b = encode_cyclical(X_b, 'date_forecast_hour', 24)\n",
    "X_b = encode_cyclical(X_b, 'date_forecast_day', 30.5)\n",
    "X_b = encode_cyclical(X_b, 'date_forecast_month', 12)\n",
    "\n",
    "X_c = encode_cyclical(X_c, 'date_forecast_hour', 24)\n",
    "X_c = encode_cyclical(X_c, 'date_forecast_day', 30.5)\n",
    "X_c = encode_cyclical(X_c, 'date_forecast_month', 12)\n",
    "\n",
    "\n",
    "#Select only some features\n",
    "X, y = process_dataset(X, y, 'A', features_to_convert)\n",
    "X_b, y_b = process_dataset(X_b, y_b, 'B', features_to_convert)\n",
    "X_c, y_c = process_dataset(X_c, y_c, 'C', features_to_convert)\n",
    "\n",
    "#score174:\n",
    "selected_features = ['is_in_shadow:idx', 'clear_sky_energy_1h:J', 'is_day:idx', 'direct_rad_1h:J', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad_1h:J', 'diffuse_rad:W', 'sun_elevation:d']\n",
    "\n",
    "#alle features\n",
    "#selected_features = ['direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'sun_elevation:d', 'is_in_shadow:idx', 'is_day:idx', 't_1000hPa:K', 'air_density_2m:kgm3', 'relative_humidity_1000hPa:p', 'dew_point_2m:K', 'absolute_humidity_2m:gm3', 'wind_speed_v_10m:ms', 'pressure_100m:hPa', 'msl_pressure:hPa', 'pressure_50m:hPa', 'effective_cloud_cover:p', 'sfc_pressure:hPa', 'visibility:m', 'ceiling_height_agl:m', 'cloud_base_agl:m', 'total_cloud_cover:p', 'snow_water:kgm2', 'super_cooled_liquid_water:kgm2', 'wind_speed_10m:ms', 'date_calc', 'fresh_snow_24h:cm', 'snow_depth:cm', 'precip_type_5min:idx', 'fresh_snow_12h:cm', 'time', 'date_forecast', 'fresh_snow_6h:cm', 'prob_rime:p', 'fresh_snow_3h:cm', 'rain_water:kgm2', 'precip_5min:mm', 'sun_azimuth:d', 'wind_speed_u_10m:ms', 'fresh_snow_1h:cm', 'dew_or_rime:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'snow_drift:idx', 'elevation:m', 'snow_density:kgm3']\n",
    "\n",
    "#selected_features = ['direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'sun_elevation:d', 'is_in_shadow:idx', 'is_day:idx', 't_1000hPa:K', 'location']\n",
    "\n",
    "\n",
    "'''X['location'] = 'A'\n",
    "X_b['location'] = 'B'\n",
    "X_c['location'] = 'C'''\n",
    "\n",
    "X = X[selected_features]\n",
    "X_b = X_b[selected_features]\n",
    "X_c = X_c[selected_features]\n",
    "\n",
    "\n",
    "# Convert float64 columns to float32\n",
    "def convert_to_float32(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "X = convert_to_float32(X)\n",
    "X_b = convert_to_float32(X_b)\n",
    "X_c = convert_to_float32(X_c)\n",
    "'''\n",
    "X, y = process_dataset(X, y, 'A', features_to_convert)\n",
    "X_b, y_b = process_dataset(X_b, y_b, 'B', features_to_convert)\n",
    "X_c, y_c = process_dataset(X_c, y_c, 'C', features_to_convert)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_transformed_c, lambda_param_c = stats.yeojohnson(y_c)\\nX_transformed_c = X_c.apply(lambda col: stats.yeojohnson(col)[0])  # Apply the transformation to each feature\\n# Convert the transformed data back to DataFrames\\ny_transformed_c = pd.Series(y_transformed_c, name=y_c.name)\\nX_transformed_c = pd.DataFrame(X_transformed_c, columns=X_c.columns)'"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transformation, we need to figure this out\n",
    "# Apply the Yeo-Johnson transformation\n",
    "'''y_transformed, lambda_param = stats.yeojohnson(y)\n",
    "X_transformed = X.apply(lambda col: stats.yeojohnson(col)[0]) # Apply the transformation to each feature\n",
    "# Convert the transformed data back to DataFrames\n",
    "y_transformed = pd.Series(y_transformed, name=y.name)\n",
    "X_transformed = pd.DataFrame(X_transformed, columns=X.columns)'''\n",
    "\n",
    "# Apply the Yeo-Johnson transformation\n",
    "'''y_transformed_b, lambda_param_b = stats.yeojohnson(y_b)\n",
    "X_transformed_b = X_b.apply(lambda col: stats.yeojohnson(col)[0])  # Apply the transformation to each feature\n",
    "# Convert the transformed data back to DataFrames\n",
    "y_transformed_b = pd.Series(y_transformed_b, name=y_b.name)\n",
    "X_transformed_b = pd.DataFrame(X_transformed_b, columns=X_b.columns)'''\n",
    "\n",
    "# Apply the Yeo-Johnson transformation\n",
    "'''y_transformed_c, lambda_param_c = stats.yeojohnson(y_c)\n",
    "X_transformed_c = X_c.apply(lambda col: stats.yeojohnson(col)[0])  # Apply the transformation to each feature\n",
    "# Convert the transformed data back to DataFrames\n",
    "y_transformed_c = pd.Series(y_transformed_c, name=y_c.name)\n",
    "X_transformed_c = pd.DataFrame(X_transformed_c, columns=X_c.columns)'''\n",
    "#X = X.drop(columns=['date_forecast'])  # Drop the original datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sns.histplot(y, kde=True, label=\\'Original\\', ax=axs[0])\\nsns.histplot(y, kde=True, label=\\'Transformed\\', ax=axs[0])\\naxs[0].legend()\\naxs[0].set_title(f\\'{\"A\"}\\')\\n\\nsns.histplot(y, kde=True, label=\\'Original\\', ax=axs[1])\\nsns.histplot(y_transformed, kde=True, label=\\'Transformed\\', ax=axs[1])\\naxs[1].legend()\\naxs[1].set_title(f\\'{\"B\"}\\')\\n\\nsns.histplot(y, kde=True, label=\\'Original\\', ax=axs[2])\\nsns.histplot(y_transformed, kde=True, label=\\'Transformed\\', ax=axs[2])\\naxs[2].legend()\\naxs[2].set_title(f\\'{\"C\"}\\')\\nplt.legend()\\nplt.show()'"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#helping polts for transformation\n",
    "# Plot the distribution of the target variable before and after the transformation\n",
    "'''sns.histplot(y, kde=True, label='Original', ax=axs[0])\n",
    "sns.histplot(y, kde=True, label='Transformed', ax=axs[0])\n",
    "axs[0].legend()\n",
    "axs[0].set_title(f'{\"A\"}')\n",
    "\n",
    "sns.histplot(y, kde=True, label='Original', ax=axs[1])\n",
    "sns.histplot(y_transformed, kde=True, label='Transformed', ax=axs[1])\n",
    "axs[1].legend()\n",
    "axs[1].set_title(f'{\"B\"}')\n",
    "\n",
    "sns.histplot(y, kde=True, label='Original', ax=axs[2])\n",
    "sns.histplot(y_transformed, kde=True, label='Transformed', ax=axs[2])\n",
    "axs[2].legend()\n",
    "axs[2].set_title(f'{\"C\"}')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(y_df,\"y_df\")\\n#log transform\\ny_df[\\'pv_measurement\\'] = y_df[\\'pv_measurement\\'].apply(lambda x: np.log(x + 1))\\nyb_df[\\'pv_measurement\\'] = yb_df[\\'pv_measurement\\'].apply(lambda x: np.log(x + 1))\\nyc_df[\\'pv_measurement\\'] = yc_df[\\'pv_measurement\\'].apply(lambda x: np.log(x + 1))\\nprint(y_df, \"log\")'"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply log transform, need to figure this out\n",
    "'''print(y_df,\"y_df\")\n",
    "#log transform\n",
    "y_df['pv_measurement'] = y_df['pv_measurement'].apply(lambda x: np.log(x + 1))\n",
    "yb_df['pv_measurement'] = yb_df['pv_measurement'].apply(lambda x: np.log(x + 1))\n",
    "yc_df['pv_measurement'] = yc_df['pv_measurement'].apply(lambda x: np.log(x + 1))\n",
    "print(y_df, \"log\")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Nan values with 0\n",
    "X.replace(np.nan,0, inplace=True)\n",
    "y.replace( np.nan,0, inplace=True)\n",
    "X_b.replace(np.nan, 0,inplace=True)\n",
    "y_b.replace(np.nan, 0,inplace=True)\n",
    "X_c.replace(np.nan, 0,inplace=True)\n",
    "y_c.replace(np.nan, 0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cat_mse = round(rmsle(y_validation, cat_prediction), 4)\\nprint('RMSLE:', cat_mse)\\nplt.figure(figsize=(6, 3), dpi=200)\\nplt.scatter(cat_prediction, y_validation)\\nplt.grid()\\nplt.show()\\n# Plot feature importances\\n\\nfeature_importance = cat_model.feature_importances_\\nsorted_idx = np.argsort(feature_importance)\\nfig = plt.figure(figsize=(8, 6))\\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\\nplt.yticks(range(len(sorted_idx)), np.array(X.columns)[sorted_idx])\\nplt.title('Feature Importance')\""
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOT IN USE\n",
    "#for now, outdated as 1 model is much better than one for each location\n",
    "'''cat_features = ['dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(int)\n",
    "X['is_day:idx'] = X['is_day:idx'].astype(int)\n",
    "X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(int)\n",
    "\n",
    "X_b['dew_or_rime:idx'] = X_b['dew_or_rime:idx'].astype(int)\n",
    "X_b['is_day:idx'] = X_b['is_day:idx'].astype(int)\n",
    "X_b['is_in_shadow:idx'] = X_b['is_in_shadow:idx'].astype(int)\n",
    "\n",
    "X_c['dew_or_rime:idx'] = X_c['dew_or_rime:idx'].astype(int)\n",
    "X_c['is_day:idx'] = X_c['is_day:idx'].astype(int)\n",
    "X_c['is_in_shadow:idx'] = X_c['is_in_shadow:idx'].astype(int)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.dtypes)\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    n_estimators=2000,\n",
    "    thread_count=-1,\n",
    "    max_depth=4,\n",
    "    silent=True,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    od_type=\"Iter\",\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "cat_model.fit(X_train, y_train, eval_set=(X_validation, y_validation), early_stopping_rounds=50)\n",
    "\n",
    "\n",
    "X_train_b, X_validation_b, y_train_b, y_validation_b = train_test_split(X_b, y_b, test_size=0.2, random_state=42)\n",
    "cat_model_b = CatBoostRegressor(\n",
    "    n_estimators=2000,\n",
    "    thread_count=-1,\n",
    "    max_depth=4,\n",
    "    silent=True,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    od_type=\"Iter\",\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "cat_model_b.fit(X_train_b, y_train_b, eval_set=(X_validation_b, y_validation_b), early_stopping_rounds=50)\n",
    "\n",
    "X_train_c, X_validation_c, y_train_c, y_validation_c = train_test_split(X_c, y_c, test_size=0.2, random_state=42)\n",
    "cat_model_c = CatBoostRegressor(\n",
    "    n_estimators=2000,\n",
    "    thread_count=-1,\n",
    "    max_depth=4,\n",
    "    silent=True,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    od_type=\"Iter\",\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "cat_model_c.fit(X_train_c, y_train_c, eval_set=(X_validation_c, y_validation_c), early_stopping_rounds=50)\n",
    "\n",
    "print(cat_model)\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "\n",
    "#grid_search = GridSearchCV(CatBoostRegressor(), param_grid, cv=5, n_jobs=-1)\n",
    "#grid_search.fit(X_train, y_train)'''\n",
    "#prediction for 3 models, outdated as of now\n",
    "\n",
    "'''# Make predictions\n",
    "y_pred_a = cat_model.predict(X_validation)\n",
    "y_pred_b = cat_model.predict(X_validation_b)\n",
    "y_pred_c = cat_model.predict(X_validation_c)\n",
    "\n",
    "# Undo the Yeo-Johnson transformation on the predictions\n",
    "y_pred_original_scale_a = (y_pred_a * lambda_param + 1)**(1/lambda_param) if lambda_param != 0 else np.exp(y_pred_a)\n",
    "y_pred_original_scale_b = (y_pred_b * lambda_param_b + 1)**(1/lambda_param_b) if lambda_param_b != 0 else np.exp(y_pred_b)\n",
    "y_pred_original_scale_c = (y_pred_c * lambda_param_c + 1)**(1/lambda_param_c) if lambda_param_c != 0 else np.exp(y_pred_c)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_a = np.sqrt(mean_squared_error(y_validation, y_pred_a))\n",
    "print(f'RMSE: {rmse_a}')\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_b = np.sqrt(mean_squared_error(y_validation_b, y_pred_b))\n",
    "print(f'RMSE: {rmse_b}')\n",
    "# Calculate the RMSE\n",
    "rmse_c = np.sqrt(mean_squared_error(y_validation_c, y_pred_c))\n",
    "print(f'RMSE: {rmse_c}')\n",
    "'''\n",
    "'''# If you want to evaluate the model on the original scale\n",
    "y_test_original_scale_a = (y_validation * lambda_param + 1)**(1/lambda_param) if lambda_param != 0 else np.exp(y_validation)\n",
    "rmse_original_scale_a = np.sqrt(mean_squared_error(y_test_original_scale_a, y_pred_original_scale_a))\n",
    "print(f'RMSE (original scale): {rmse_original_scale_a}')\n",
    "\n",
    "# If you want to evaluate the model on the original scale\n",
    "y_test_original_scale_b = (y_validation_b * lambda_param_b + 1)**(1/lambda_param_b) if lambda_param_b != 0 else np.exp(y_validation_b)\n",
    "rmse_original_scale_b = np.sqrt(mean_squared_error(y_test_original_scale_b, y_pred_original_scale_b))\n",
    "print(f'RMSE (original scale): {rmse_original_scale_b}')\n",
    "# If you want to evaluate the model on the original scale\n",
    "y_test_original_scale_c = (y_pred_original_scale_c * lambda_param_c + 1)**(1/lambda_param_c) if lambda_param_c != 0 else np.exp(y_pred_original_scale_c)\n",
    "rmse_original_scale_c = np.sqrt(mean_squared_error(y_test_original_scale_c, y_pred_original_scale_c))\n",
    "print(f'RMSE (original scale): {rmse_original_scale_c}')'''\n",
    "'''RMSE: 314.78827508863765\n",
    "RMSE: 997.4962753104166\n",
    "RMSE: 1059.2958848252413'''\n",
    "#NOT IN USE\n",
    "#prediction of the 3 catboost model, outdated\n",
    "#print(X_test_estimated_a.dtypes)\n",
    "\n",
    "\n",
    "'''X_test_a = X_test_estimated_a.drop(columns=[ 'date_calc', 'date_forecast'])#,  'elevation:m', 'wind_speed_w_1000hPa:ms','snow_drift:idx', 'snow_density:kgm3'])\n",
    "X_test_b = X_test_estimated_b.drop(columns=[ 'date_calc', 'date_forecast']) #, 'elevation:m', 'wind_speed_w_1000hPa:ms','snow_drift:idx', 'snow_density:kgm3'])\n",
    "X_test_c = X_test_estimated_c.drop(columns=[  'date_calc', 'date_forecast'])# , 'elevation:m', 'wind_speed_w_1000hPa:ms','snow_drift:idx', 'snow_density:kgm3'])\n",
    "\n",
    "\n",
    "# List of features to convert to categorical\n",
    "features_to_convert = ['dew_or_rime:idx', 'is_day:idx','is_in_shadow:idx']\n",
    "\n",
    "# Convert features for X\n",
    "for feature in features_to_convert:\n",
    "    X_test_a[feature] = X_test_a[feature].astype(int)\n",
    "\n",
    "# Convert features for X_b\n",
    "for feature in features_to_convert:\n",
    "    X_test_b[feature] = X_test_b[feature].astype(int)\n",
    "\n",
    "# Convert features for X_c\n",
    "for feature in features_to_convert:\n",
    "    X_test_c[feature] = X_test_c[feature].astype(int)\n",
    "print(X_test_a.dtypes)\n",
    "\n",
    "cat_prediction = cat_model.predict(X_test_a)\n",
    "\n",
    "cat_prediction_b = cat_model.predict(X_test_b)\n",
    "\n",
    "cat_prediction_c = cat_model.predict(X_test_c)\n",
    "print(cat_prediction,\"before\")'''\n",
    "#undo log transform\n",
    "'''cat_prediction = np.exp(cat_prediction) - 1\n",
    "cat_prediction_b = np.exp(cat_prediction_b) - 1\n",
    "cat_prediction_c = np.exp(cat_prediction_c) - 1'''\n",
    "#print(cat_prediction,\"after\")\n",
    "'''print(len(cat_prediction))\n",
    "print(len(cat_prediction_b))\n",
    "print(len(cat_prediction_c))'''\n",
    "#bruke lag features\n",
    "# Set negative values to mean\n",
    "'''cat_prediction[cat_prediction < 0] = 0\n",
    "\n",
    "cat_prediction_b[cat_prediction_b < 0] = 0\n",
    "\n",
    "cat_prediction_c[cat_prediction_c < 0] = 0'''\n",
    "\n",
    "\n",
    "'''hourly_prediction_a = cat_prediction[::4]\n",
    "#print(len(hourly_prediction_a))\n",
    "hourly_prediction_b = cat_prediction_b[::4]\n",
    "hourly_prediction_c = cat_prediction_c[::4]\n",
    "num_predictions = len(hourly_prediction_a)\n",
    "timestamps = pd.date_range(start='2023-05-01 00:00:00', periods=num_predictions, freq='H')\n",
    "# Step 1: Create individual DataFrames for each location\n",
    "df_a = pd.DataFrame({\n",
    "    'time': timestamps,\n",
    "    'prediction': hourly_prediction_a,\n",
    "    'location': 'A'\n",
    "})\n",
    "\n",
    "df_b = pd.DataFrame({\n",
    "    'time': timestamps,\n",
    "    'prediction': hourly_prediction_b,\n",
    "    'location': 'B'\n",
    "})\n",
    "\n",
    "df_c = pd.DataFrame({\n",
    "    'time': timestamps,\n",
    "    'prediction': hourly_prediction_c,\n",
    "    'location': 'C'\n",
    "})\n",
    "#print(df_a)\n",
    "\n",
    "# Step 2: Concatenate these DataFrames\n",
    "final_df = pd.concat([df_a, df_b, df_c], ignore_index=True)\n",
    "\n",
    "# Step 3: Reset the index to get the 'id' column\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "# Now final_df is in the desired format\n",
    "#print(final_df)\n",
    "#print(final_df.columns)\n",
    "final_df= final_df.drop(columns=['time', 'location'])\n",
    "\n",
    "#print(final_df)\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)\n",
    "'''\n",
    "\n",
    "\n",
    "'''cat_mse = round(rmsle(y_validation, cat_prediction), 4)\n",
    "print('RMSLE:', cat_mse)\n",
    "plt.figure(figsize=(6, 3), dpi=200)\n",
    "plt.scatter(cat_prediction, y_validation)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Plot feature importances\n",
    "\n",
    "feature_importance = cat_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X.columns)[sorted_idx])\n",
    "plt.title('Feature Importance')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_in_shadow:idx           int64\n",
      "clear_sky_energy_1h:J    float32\n",
      "is_day:idx                 int64\n",
      "direct_rad_1h:J          float32\n",
      "clear_sky_rad:W          float32\n",
      "direct_rad:W             float32\n",
      "diffuse_rad_1h:J         float32\n",
      "diffuse_rad:W            float32\n",
      "sun_elevation:d          float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#1 model instead of 3, CaaaaaaaaatBooooozzzzt\n",
    "\n",
    "# Add location column\n",
    "'''X['location'] = 'A'\n",
    "X_b['location'] = 'B'\n",
    "X_c['location'] = 'C'''\n",
    "\n",
    "# List of features to convert to categorical\n",
    "features_to_convert = ['is_day:idx','is_in_shadow:idx']\n",
    "\n",
    "# Convert features for X\n",
    "for feature in features_to_convert:\n",
    "    X[feature] = X[feature].astype(int)\n",
    "\n",
    "# Convert features for X_b\n",
    "for feature in features_to_convert:\n",
    "    X_b[feature] = X_b[feature].astype(int)\n",
    "\n",
    "# Convert features for X_c\n",
    "for feature in features_to_convert:\n",
    "    X_c[feature] = X_c[feature].astype(int)\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X, X_b, X_c])\n",
    "y_combined = pd.concat([y, y_b, y_c])\n",
    "\n",
    "#assert 'location' in X_combined.columns, \"Location column is missing in combined dataset\"\n",
    "\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train_combined, X_validation_combined, y_train_combined, y_validation_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train_combined.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Learning rate set to 0.066279\n",
      "0:\tlearn: 730.3541780\ttest: 721.5046425\tbest: 721.5046425 (0)\ttotal: 10.5ms\tremaining: 21.1s\n",
      "1:\tlearn: 714.6617078\ttest: 705.8498702\tbest: 705.8498702 (1)\ttotal: 13.5ms\tremaining: 13.4s\n",
      "2:\tlearn: 700.8212647\ttest: 691.9834517\tbest: 691.9834517 (2)\ttotal: 17.4ms\tremaining: 11.6s\n",
      "3:\tlearn: 688.5388607\ttest: 679.7151880\tbest: 679.7151880 (3)\ttotal: 19.9ms\tremaining: 9.91s\n",
      "4:\tlearn: 677.3486597\ttest: 668.5915355\tbest: 668.5915355 (4)\ttotal: 22.6ms\tremaining: 9s\n",
      "5:\tlearn: 667.3512091\ttest: 658.6842507\tbest: 658.6842507 (5)\ttotal: 24.9ms\tremaining: 8.27s\n",
      "6:\tlearn: 658.7423043\ttest: 650.1264549\tbest: 650.1264549 (6)\ttotal: 27.2ms\tremaining: 7.75s\n",
      "7:\tlearn: 650.7623559\ttest: 642.1906286\tbest: 642.1906286 (7)\ttotal: 29.8ms\tremaining: 7.42s\n",
      "8:\tlearn: 643.7804880\ttest: 635.3586071\tbest: 635.3586071 (8)\ttotal: 32.2ms\tremaining: 7.12s\n",
      "9:\tlearn: 637.4261545\ttest: 629.0994191\tbest: 629.0994191 (9)\ttotal: 34.9ms\tremaining: 6.94s\n",
      "10:\tlearn: 631.9563962\ttest: 623.6705838\tbest: 623.6705838 (10)\ttotal: 37.3ms\tremaining: 6.75s\n",
      "11:\tlearn: 627.1485469\ttest: 619.0407724\tbest: 619.0407724 (11)\ttotal: 39.8ms\tremaining: 6.6s\n",
      "12:\tlearn: 622.6326345\ttest: 614.6148676\tbest: 614.6148676 (12)\ttotal: 42.6ms\tremaining: 6.52s\n",
      "13:\tlearn: 618.9778516\ttest: 611.1148967\tbest: 611.1148967 (13)\ttotal: 45.6ms\tremaining: 6.47s\n",
      "14:\tlearn: 615.4857293\ttest: 607.8429015\tbest: 607.8429015 (14)\ttotal: 47.9ms\tremaining: 6.34s\n",
      "15:\tlearn: 612.3690493\ttest: 604.8027300\tbest: 604.8027300 (15)\ttotal: 50.4ms\tremaining: 6.25s\n",
      "16:\tlearn: 609.6747240\ttest: 602.1751891\tbest: 602.1751891 (16)\ttotal: 52.8ms\tremaining: 6.16s\n",
      "17:\tlearn: 607.2763712\ttest: 599.9374260\tbest: 599.9374260 (17)\ttotal: 55.3ms\tremaining: 6.08s\n",
      "18:\tlearn: 605.1245578\ttest: 597.8902225\tbest: 597.8902225 (18)\ttotal: 57.7ms\tremaining: 6.01s\n",
      "19:\tlearn: 603.2296138\ttest: 596.0720972\tbest: 596.0720972 (19)\ttotal: 59.9ms\tremaining: 5.93s\n",
      "20:\tlearn: 601.6010309\ttest: 594.5188667\tbest: 594.5188667 (20)\ttotal: 63.1ms\tremaining: 5.94s\n",
      "21:\tlearn: 600.0715526\ttest: 593.0848602\tbest: 593.0848602 (21)\ttotal: 65.8ms\tremaining: 5.91s\n",
      "22:\tlearn: 598.7504870\ttest: 591.8375226\tbest: 591.8375226 (22)\ttotal: 68.7ms\tremaining: 5.9s\n",
      "23:\tlearn: 597.5624253\ttest: 590.7654563\tbest: 590.7654563 (23)\ttotal: 71.5ms\tremaining: 5.89s\n",
      "24:\tlearn: 596.5581711\ttest: 589.8183715\tbest: 589.8183715 (24)\ttotal: 76.5ms\tremaining: 6.04s\n",
      "25:\tlearn: 595.6291936\ttest: 588.9421061\tbest: 588.9421061 (25)\ttotal: 80.2ms\tremaining: 6.09s\n",
      "26:\tlearn: 594.8412472\ttest: 588.2405174\tbest: 588.2405174 (26)\ttotal: 84.2ms\tremaining: 6.16s\n",
      "27:\tlearn: 594.1543516\ttest: 587.6208013\tbest: 587.6208013 (27)\ttotal: 87.4ms\tremaining: 6.15s\n",
      "28:\tlearn: 593.5812836\ttest: 587.0867921\tbest: 587.0867921 (28)\ttotal: 90.7ms\tremaining: 6.17s\n",
      "29:\tlearn: 592.9781237\ttest: 586.5907951\tbest: 586.5907951 (29)\ttotal: 93.8ms\tremaining: 6.16s\n",
      "30:\tlearn: 592.4577709\ttest: 586.1428350\tbest: 586.1428350 (30)\ttotal: 99.8ms\tremaining: 6.34s\n",
      "31:\tlearn: 591.9700310\ttest: 585.7902643\tbest: 585.7902643 (31)\ttotal: 103ms\tremaining: 6.34s\n",
      "32:\tlearn: 591.5484577\ttest: 585.4659096\tbest: 585.4659096 (32)\ttotal: 106ms\tremaining: 6.32s\n",
      "33:\tlearn: 591.1686008\ttest: 585.1861839\tbest: 585.1861839 (33)\ttotal: 109ms\tremaining: 6.33s\n",
      "34:\tlearn: 590.8347731\ttest: 584.9389933\tbest: 584.9389933 (34)\ttotal: 114ms\tremaining: 6.38s\n",
      "35:\tlearn: 590.5376803\ttest: 584.6842992\tbest: 584.6842992 (35)\ttotal: 117ms\tremaining: 6.38s\n",
      "36:\tlearn: 590.2697247\ttest: 584.4347051\tbest: 584.4347051 (36)\ttotal: 120ms\tremaining: 6.36s\n",
      "37:\tlearn: 590.0089977\ttest: 584.2478656\tbest: 584.2478656 (37)\ttotal: 123ms\tremaining: 6.34s\n",
      "38:\tlearn: 589.7372928\ttest: 584.0977623\tbest: 584.0977623 (38)\ttotal: 126ms\tremaining: 6.32s\n",
      "39:\tlearn: 589.5449102\ttest: 583.9441411\tbest: 583.9441411 (39)\ttotal: 129ms\tremaining: 6.32s\n",
      "40:\tlearn: 589.3488254\ttest: 583.8111981\tbest: 583.8111981 (40)\ttotal: 133ms\tremaining: 6.33s\n",
      "41:\tlearn: 589.1664729\ttest: 583.7168745\tbest: 583.7168745 (41)\ttotal: 135ms\tremaining: 6.31s\n",
      "42:\tlearn: 588.9549867\ttest: 583.6365385\tbest: 583.6365385 (42)\ttotal: 139ms\tremaining: 6.31s\n",
      "43:\tlearn: 588.7864843\ttest: 583.5707460\tbest: 583.5707460 (43)\ttotal: 142ms\tremaining: 6.3s\n",
      "44:\tlearn: 588.6363115\ttest: 583.4785149\tbest: 583.4785149 (44)\ttotal: 144ms\tremaining: 6.28s\n",
      "45:\tlearn: 588.5012912\ttest: 583.3649052\tbest: 583.3649052 (45)\ttotal: 147ms\tremaining: 6.25s\n",
      "46:\tlearn: 588.3632546\ttest: 583.3190548\tbest: 583.3190548 (46)\ttotal: 149ms\tremaining: 6.21s\n",
      "47:\tlearn: 588.2535847\ttest: 583.2246517\tbest: 583.2246517 (47)\ttotal: 153ms\tremaining: 6.21s\n",
      "48:\tlearn: 588.1133834\ttest: 583.1518818\tbest: 583.1518818 (48)\ttotal: 155ms\tremaining: 6.19s\n",
      "49:\tlearn: 588.0058565\ttest: 583.0797733\tbest: 583.0797733 (49)\ttotal: 158ms\tremaining: 6.17s\n",
      "50:\tlearn: 587.8736987\ttest: 582.9911863\tbest: 582.9911863 (50)\ttotal: 161ms\tremaining: 6.14s\n",
      "51:\tlearn: 587.7781960\ttest: 582.9097452\tbest: 582.9097452 (51)\ttotal: 164ms\tremaining: 6.15s\n",
      "52:\tlearn: 587.6890455\ttest: 582.8484302\tbest: 582.8484302 (52)\ttotal: 169ms\tremaining: 6.2s\n",
      "53:\tlearn: 587.6205910\ttest: 582.8261933\tbest: 582.8261933 (53)\ttotal: 171ms\tremaining: 6.18s\n",
      "54:\tlearn: 587.4998082\ttest: 582.7811747\tbest: 582.7811747 (54)\ttotal: 175ms\tremaining: 6.18s\n",
      "55:\tlearn: 587.4345335\ttest: 582.7369255\tbest: 582.7369255 (55)\ttotal: 179ms\tremaining: 6.21s\n",
      "56:\tlearn: 587.3675264\ttest: 582.6830727\tbest: 582.6830727 (56)\ttotal: 182ms\tremaining: 6.21s\n",
      "57:\tlearn: 587.3012014\ttest: 582.6556569\tbest: 582.6556569 (57)\ttotal: 185ms\tremaining: 6.2s\n",
      "58:\tlearn: 587.2303583\ttest: 582.6442882\tbest: 582.6442882 (58)\ttotal: 188ms\tremaining: 6.18s\n",
      "59:\tlearn: 587.1334920\ttest: 582.6846309\tbest: 582.6442882 (58)\ttotal: 190ms\tremaining: 6.15s\n",
      "60:\tlearn: 587.0564802\ttest: 582.6605983\tbest: 582.6442882 (58)\ttotal: 193ms\tremaining: 6.13s\n",
      "61:\tlearn: 586.9442216\ttest: 582.6827059\tbest: 582.6442882 (58)\ttotal: 195ms\tremaining: 6.1s\n",
      "62:\tlearn: 586.8767104\ttest: 582.6349167\tbest: 582.6349167 (62)\ttotal: 198ms\tremaining: 6.08s\n",
      "63:\tlearn: 586.8065284\ttest: 582.6067855\tbest: 582.6067855 (63)\ttotal: 201ms\tremaining: 6.07s\n",
      "64:\tlearn: 586.7561410\ttest: 582.5571355\tbest: 582.5571355 (64)\ttotal: 203ms\tremaining: 6.06s\n",
      "65:\tlearn: 586.6681272\ttest: 582.5124484\tbest: 582.5124484 (65)\ttotal: 206ms\tremaining: 6.04s\n",
      "66:\tlearn: 586.5862163\ttest: 582.5260916\tbest: 582.5124484 (65)\ttotal: 209ms\tremaining: 6.02s\n",
      "67:\tlearn: 586.5163791\ttest: 582.4872603\tbest: 582.4872603 (67)\ttotal: 211ms\tremaining: 6s\n",
      "68:\tlearn: 586.4343079\ttest: 582.4812807\tbest: 582.4812807 (68)\ttotal: 214ms\tremaining: 5.98s\n",
      "69:\tlearn: 586.3557201\ttest: 582.4672732\tbest: 582.4672732 (69)\ttotal: 216ms\tremaining: 5.97s\n",
      "70:\tlearn: 586.2792512\ttest: 582.4494685\tbest: 582.4494685 (70)\ttotal: 219ms\tremaining: 5.95s\n",
      "71:\tlearn: 586.1888814\ttest: 582.3815257\tbest: 582.3815257 (71)\ttotal: 222ms\tremaining: 5.94s\n",
      "72:\tlearn: 586.0947645\ttest: 582.3616789\tbest: 582.3616789 (72)\ttotal: 224ms\tremaining: 5.92s\n",
      "73:\tlearn: 586.0706127\ttest: 582.3662555\tbest: 582.3616789 (72)\ttotal: 227ms\tremaining: 5.9s\n",
      "74:\tlearn: 586.0235950\ttest: 582.3441204\tbest: 582.3441204 (74)\ttotal: 229ms\tremaining: 5.88s\n",
      "75:\tlearn: 585.9774014\ttest: 582.3040438\tbest: 582.3040438 (75)\ttotal: 232ms\tremaining: 5.88s\n",
      "76:\tlearn: 585.9114246\ttest: 582.3418432\tbest: 582.3040438 (75)\ttotal: 235ms\tremaining: 5.86s\n",
      "77:\tlearn: 585.8443280\ttest: 582.3234497\tbest: 582.3040438 (75)\ttotal: 237ms\tremaining: 5.84s\n",
      "78:\tlearn: 585.8144004\ttest: 582.2954540\tbest: 582.2954540 (78)\ttotal: 240ms\tremaining: 5.83s\n",
      "79:\tlearn: 585.7483652\ttest: 582.2962876\tbest: 582.2954540 (78)\ttotal: 242ms\tremaining: 5.81s\n",
      "80:\tlearn: 585.7165128\ttest: 582.2954094\tbest: 582.2954094 (80)\ttotal: 245ms\tremaining: 5.79s\n",
      "81:\tlearn: 585.6194106\ttest: 582.2956783\tbest: 582.2954094 (80)\ttotal: 247ms\tremaining: 5.78s\n",
      "82:\tlearn: 585.5367571\ttest: 582.2858648\tbest: 582.2858648 (82)\ttotal: 250ms\tremaining: 5.76s\n",
      "83:\tlearn: 585.4661464\ttest: 582.2795141\tbest: 582.2795141 (83)\ttotal: 252ms\tremaining: 5.75s\n",
      "84:\tlearn: 585.3814413\ttest: 582.2756613\tbest: 582.2756613 (84)\ttotal: 255ms\tremaining: 5.74s\n",
      "85:\tlearn: 585.3493823\ttest: 582.3170514\tbest: 582.2756613 (84)\ttotal: 258ms\tremaining: 5.73s\n",
      "86:\tlearn: 585.2975894\ttest: 582.3716975\tbest: 582.2756613 (84)\ttotal: 260ms\tremaining: 5.73s\n",
      "87:\tlearn: 585.2356491\ttest: 582.3600497\tbest: 582.2756613 (84)\ttotal: 263ms\tremaining: 5.71s\n",
      "88:\tlearn: 585.1944530\ttest: 582.3372209\tbest: 582.2756613 (84)\ttotal: 266ms\tremaining: 5.7s\n",
      "89:\tlearn: 585.1433710\ttest: 582.3371866\tbest: 582.2756613 (84)\ttotal: 268ms\tremaining: 5.69s\n",
      "90:\tlearn: 585.1223448\ttest: 582.3193361\tbest: 582.2756613 (84)\ttotal: 272ms\tremaining: 5.71s\n",
      "91:\tlearn: 585.0881364\ttest: 582.2926499\tbest: 582.2756613 (84)\ttotal: 276ms\tremaining: 5.72s\n",
      "92:\tlearn: 585.0492268\ttest: 582.2684429\tbest: 582.2684429 (92)\ttotal: 279ms\tremaining: 5.72s\n",
      "93:\tlearn: 584.9801435\ttest: 582.2727923\tbest: 582.2684429 (92)\ttotal: 282ms\tremaining: 5.71s\n",
      "94:\tlearn: 584.9370819\ttest: 582.2359673\tbest: 582.2359673 (94)\ttotal: 285ms\tremaining: 5.71s\n",
      "95:\tlearn: 584.9141158\ttest: 582.2538228\tbest: 582.2359673 (94)\ttotal: 288ms\tremaining: 5.71s\n",
      "96:\tlearn: 584.8246859\ttest: 582.2734926\tbest: 582.2359673 (94)\ttotal: 291ms\tremaining: 5.7s\n",
      "97:\tlearn: 584.7733180\ttest: 582.2306415\tbest: 582.2306415 (97)\ttotal: 293ms\tremaining: 5.7s\n",
      "98:\tlearn: 584.7515197\ttest: 582.2197278\tbest: 582.2197278 (98)\ttotal: 296ms\tremaining: 5.69s\n",
      "99:\tlearn: 584.7140702\ttest: 582.2003956\tbest: 582.2003956 (99)\ttotal: 299ms\tremaining: 5.68s\n",
      "100:\tlearn: 584.6346711\ttest: 582.1537537\tbest: 582.1537537 (100)\ttotal: 302ms\tremaining: 5.67s\n",
      "101:\tlearn: 584.5890077\ttest: 582.1766362\tbest: 582.1537537 (100)\ttotal: 304ms\tremaining: 5.66s\n",
      "102:\tlearn: 584.5310819\ttest: 582.1717681\tbest: 582.1537537 (100)\ttotal: 307ms\tremaining: 5.65s\n",
      "103:\tlearn: 584.4885330\ttest: 582.1412943\tbest: 582.1412943 (103)\ttotal: 309ms\tremaining: 5.64s\n",
      "104:\tlearn: 584.4690938\ttest: 582.1608278\tbest: 582.1412943 (103)\ttotal: 312ms\tremaining: 5.64s\n",
      "105:\tlearn: 584.4244070\ttest: 582.1563866\tbest: 582.1412943 (103)\ttotal: 315ms\tremaining: 5.63s\n",
      "106:\tlearn: 584.3509497\ttest: 582.1528380\tbest: 582.1412943 (103)\ttotal: 318ms\tremaining: 5.62s\n",
      "107:\tlearn: 584.2890559\ttest: 582.1520721\tbest: 582.1412943 (103)\ttotal: 321ms\tremaining: 5.62s\n",
      "108:\tlearn: 584.2357286\ttest: 582.1170282\tbest: 582.1170282 (108)\ttotal: 323ms\tremaining: 5.61s\n",
      "109:\tlearn: 584.2016806\ttest: 582.0905555\tbest: 582.0905555 (109)\ttotal: 326ms\tremaining: 5.6s\n",
      "110:\tlearn: 584.1852694\ttest: 582.0757252\tbest: 582.0757252 (110)\ttotal: 329ms\tremaining: 5.6s\n",
      "111:\tlearn: 584.1075984\ttest: 582.0983577\tbest: 582.0757252 (110)\ttotal: 331ms\tremaining: 5.59s\n",
      "112:\tlearn: 584.0737023\ttest: 582.0703762\tbest: 582.0703762 (112)\ttotal: 335ms\tremaining: 5.59s\n",
      "113:\tlearn: 584.0180309\ttest: 582.0592708\tbest: 582.0592708 (113)\ttotal: 338ms\tremaining: 5.58s\n",
      "114:\tlearn: 583.9666257\ttest: 582.0575369\tbest: 582.0575369 (114)\ttotal: 340ms\tremaining: 5.58s\n",
      "115:\tlearn: 583.8835379\ttest: 582.1020362\tbest: 582.0575369 (114)\ttotal: 343ms\tremaining: 5.57s\n",
      "116:\tlearn: 583.8649687\ttest: 582.0922487\tbest: 582.0575369 (114)\ttotal: 347ms\tremaining: 5.58s\n",
      "117:\tlearn: 583.8000682\ttest: 582.0988580\tbest: 582.0575369 (114)\ttotal: 350ms\tremaining: 5.58s\n",
      "118:\tlearn: 583.7684864\ttest: 582.0801086\tbest: 582.0575369 (114)\ttotal: 352ms\tremaining: 5.57s\n",
      "119:\tlearn: 583.7141842\ttest: 582.0471066\tbest: 582.0471066 (119)\ttotal: 355ms\tremaining: 5.57s\n",
      "120:\tlearn: 583.6722831\ttest: 582.0071038\tbest: 582.0071038 (120)\ttotal: 359ms\tremaining: 5.57s\n",
      "121:\tlearn: 583.6019367\ttest: 581.9838216\tbest: 581.9838216 (121)\ttotal: 362ms\tremaining: 5.58s\n",
      "122:\tlearn: 583.5302073\ttest: 581.9694229\tbest: 581.9694229 (122)\ttotal: 366ms\tremaining: 5.58s\n",
      "123:\tlearn: 583.4627558\ttest: 582.0400010\tbest: 581.9694229 (122)\ttotal: 369ms\tremaining: 5.58s\n",
      "124:\tlearn: 583.4157595\ttest: 582.0285701\tbest: 581.9694229 (122)\ttotal: 371ms\tremaining: 5.57s\n",
      "125:\tlearn: 583.4044453\ttest: 582.0207992\tbest: 581.9694229 (122)\ttotal: 375ms\tremaining: 5.57s\n",
      "126:\tlearn: 583.3668189\ttest: 582.0323882\tbest: 581.9694229 (122)\ttotal: 377ms\tremaining: 5.56s\n",
      "127:\tlearn: 583.2979904\ttest: 582.0806864\tbest: 581.9694229 (122)\ttotal: 380ms\tremaining: 5.55s\n",
      "128:\tlearn: 583.2628499\ttest: 582.0546876\tbest: 581.9694229 (122)\ttotal: 383ms\tremaining: 5.55s\n",
      "129:\tlearn: 583.2042954\ttest: 582.0502675\tbest: 581.9694229 (122)\ttotal: 386ms\tremaining: 5.55s\n",
      "130:\tlearn: 583.1663302\ttest: 582.0517421\tbest: 581.9694229 (122)\ttotal: 388ms\tremaining: 5.54s\n",
      "131:\tlearn: 583.1156271\ttest: 582.0451191\tbest: 581.9694229 (122)\ttotal: 391ms\tremaining: 5.54s\n",
      "132:\tlearn: 583.0823347\ttest: 582.1088679\tbest: 581.9694229 (122)\ttotal: 394ms\tremaining: 5.53s\n",
      "133:\tlearn: 583.0649622\ttest: 582.1012731\tbest: 581.9694229 (122)\ttotal: 397ms\tremaining: 5.52s\n",
      "134:\tlearn: 583.0397408\ttest: 582.0911775\tbest: 581.9694229 (122)\ttotal: 399ms\tremaining: 5.51s\n",
      "135:\tlearn: 582.9940398\ttest: 582.0820006\tbest: 581.9694229 (122)\ttotal: 402ms\tremaining: 5.51s\n",
      "136:\tlearn: 582.9719840\ttest: 582.0617843\tbest: 581.9694229 (122)\ttotal: 404ms\tremaining: 5.5s\n",
      "137:\tlearn: 582.9311243\ttest: 582.0122204\tbest: 581.9694229 (122)\ttotal: 407ms\tremaining: 5.5s\n",
      "138:\tlearn: 582.8933095\ttest: 581.9958128\tbest: 581.9694229 (122)\ttotal: 410ms\tremaining: 5.49s\n",
      "139:\tlearn: 582.8370869\ttest: 582.0093864\tbest: 581.9694229 (122)\ttotal: 413ms\tremaining: 5.49s\n",
      "140:\tlearn: 582.7667425\ttest: 581.9490762\tbest: 581.9490762 (140)\ttotal: 416ms\tremaining: 5.48s\n",
      "141:\tlearn: 582.7191797\ttest: 581.9727753\tbest: 581.9490762 (140)\ttotal: 418ms\tremaining: 5.47s\n",
      "142:\tlearn: 582.6835862\ttest: 582.0038477\tbest: 581.9490762 (140)\ttotal: 421ms\tremaining: 5.46s\n",
      "143:\tlearn: 582.6452179\ttest: 581.9719943\tbest: 581.9490762 (140)\ttotal: 423ms\tremaining: 5.46s\n",
      "144:\tlearn: 582.6095162\ttest: 581.9891688\tbest: 581.9490762 (140)\ttotal: 426ms\tremaining: 5.45s\n",
      "145:\tlearn: 582.5736715\ttest: 581.9561934\tbest: 581.9490762 (140)\ttotal: 429ms\tremaining: 5.44s\n",
      "146:\tlearn: 582.5483978\ttest: 581.9430793\tbest: 581.9430793 (146)\ttotal: 431ms\tremaining: 5.43s\n",
      "147:\tlearn: 582.4946374\ttest: 581.9347520\tbest: 581.9347520 (147)\ttotal: 434ms\tremaining: 5.43s\n",
      "148:\tlearn: 582.4606168\ttest: 581.9253539\tbest: 581.9253539 (148)\ttotal: 436ms\tremaining: 5.42s\n",
      "149:\tlearn: 582.4025792\ttest: 581.9517542\tbest: 581.9253539 (148)\ttotal: 439ms\tremaining: 5.41s\n",
      "150:\tlearn: 582.3734997\ttest: 581.9466560\tbest: 581.9253539 (148)\ttotal: 442ms\tremaining: 5.41s\n",
      "151:\tlearn: 582.3314986\ttest: 581.9580885\tbest: 581.9253539 (148)\ttotal: 444ms\tremaining: 5.4s\n",
      "152:\tlearn: 582.3011048\ttest: 581.9962958\tbest: 581.9253539 (148)\ttotal: 447ms\tremaining: 5.4s\n",
      "153:\tlearn: 582.2819196\ttest: 581.9939464\tbest: 581.9253539 (148)\ttotal: 450ms\tremaining: 5.4s\n",
      "154:\tlearn: 582.2451112\ttest: 581.9910315\tbest: 581.9253539 (148)\ttotal: 453ms\tremaining: 5.39s\n",
      "155:\tlearn: 582.1768940\ttest: 581.9967658\tbest: 581.9253539 (148)\ttotal: 456ms\tremaining: 5.39s\n",
      "156:\tlearn: 582.1641093\ttest: 581.9911764\tbest: 581.9253539 (148)\ttotal: 459ms\tremaining: 5.38s\n",
      "157:\tlearn: 582.1316504\ttest: 581.9705503\tbest: 581.9253539 (148)\ttotal: 462ms\tremaining: 5.38s\n",
      "158:\tlearn: 582.0945653\ttest: 581.9541323\tbest: 581.9253539 (148)\ttotal: 465ms\tremaining: 5.39s\n",
      "159:\tlearn: 582.0411497\ttest: 581.9718094\tbest: 581.9253539 (148)\ttotal: 468ms\tremaining: 5.39s\n",
      "160:\tlearn: 582.0190473\ttest: 582.0124863\tbest: 581.9253539 (148)\ttotal: 471ms\tremaining: 5.38s\n",
      "161:\tlearn: 581.9926134\ttest: 582.0027093\tbest: 581.9253539 (148)\ttotal: 474ms\tremaining: 5.37s\n",
      "162:\tlearn: 581.9595860\ttest: 581.9892579\tbest: 581.9253539 (148)\ttotal: 476ms\tremaining: 5.37s\n",
      "163:\tlearn: 581.8971884\ttest: 582.0319904\tbest: 581.9253539 (148)\ttotal: 479ms\tremaining: 5.36s\n",
      "164:\tlearn: 581.8783268\ttest: 582.0269765\tbest: 581.9253539 (148)\ttotal: 483ms\tremaining: 5.37s\n",
      "165:\tlearn: 581.8411258\ttest: 582.0662254\tbest: 581.9253539 (148)\ttotal: 485ms\tremaining: 5.36s\n",
      "166:\tlearn: 581.8094327\ttest: 582.1067520\tbest: 581.9253539 (148)\ttotal: 488ms\tremaining: 5.35s\n",
      "167:\tlearn: 581.7805478\ttest: 582.0904446\tbest: 581.9253539 (148)\ttotal: 491ms\tremaining: 5.35s\n",
      "168:\tlearn: 581.7042541\ttest: 582.1111193\tbest: 581.9253539 (148)\ttotal: 493ms\tremaining: 5.34s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 581.9253539\n",
      "bestIteration = 148\n",
      "\n",
      "Shrink model to first 149 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x13d5b4cd0>"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 model instead of 3\n",
    "\n",
    "#adding location to cat_features\n",
    "cat_features = ['is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "\n",
    "cat_model_combined = CatBoostRegressor(\n",
    "    n_estimators=2000,\n",
    "    thread_count=-1,\n",
    "    max_depth=4,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    od_type=\"Iter\",\n",
    "    logging_level='Verbose',\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "\n",
    "selected_features = ['is_in_shadow:idx', 'clear_sky_energy_1h:J', 'is_day:idx', 'direct_rad_1h:J', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad_1h:J', 'diffuse_rad:W', 'sun_elevation:d']\n",
    "\n",
    "#selected_features = ['is_in_shadow:idx', 'clear_sky_energy_1h:J', 'is_day:idx', 'direct_rad_1h:J', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad_1h:J', 'diffuse_rad:W', 'sun_elevation:d', 'location']\n",
    "#selected_features = ['direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'sun_elevation:d', 'is_in_shadow:idx', 'is_day:idx', 't_1000hPa:K', 'air_density_2m:kgm3', 'relative_humidity_1000hPa:p', 'dew_point_2m:K', 'absolute_humidity_2m:gm3', 'wind_speed_v_10m:ms']\n",
    "#selected_features = ['direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'sun_elevation:d', 'is_in_shadow:idx', 'is_day:idx', 't_1000hPa:K', 'location']\n",
    "\n",
    "print(y_train_combined.dtypes)\n",
    "\n",
    "\n",
    "# Train the model with the specified feature types\n",
    "cat_model_combined.fit(X_train_combined, y_train_combined, cat_features=cat_features, eval_set=(X_validation_combined, y_validation_combined))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features_to_convert = [\\'is_day:idx\\', \\'is_in_shadow:idx\\']\\n\\n# Convert features for X_test_a\\nfor feature in features_to_convert:\\n    X_test_a[feature] = X_test_a[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\\n\\n# Convert features for X_test_b\\nfor feature in features_to_convert:\\n    X_test_b[feature] = X_test_b[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\\n\\n# Convert features for X_test_c\\nfor feature in features_to_convert:\\n    X_test_c[feature] = X_test_c[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\\n\\nnan_replacement = \"missing_value\"\\n\\n# Replace NaN values in categorical features for X_test_a, X_test_b, and X_test_c\\nfor feature in cat_features:\\n    X_test_a[feature].fillna(nan_replacement, inplace=True)\\n    X_test_b[feature].fillna(nan_replacement, inplace=True)\\n    X_test_c[feature].fillna(nan_replacement, inplace=True)\\n\\n    \\n    # Convert float64 columns to float32\\ndef convert_to_float32(df):\\n    for col in df.columns:\\n        if df[col].dtype == \\'float64\\':\\n            df[col] = df[col].astype(\\'float32\\')\\n    return df\\n\\nX_test_a = convert_to_float32(X_test_a)\\nX_test_b = convert_to_float32(X_test_b)\\nX_test_c = convert_to_float32(X_test_c)\\n'"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#managing the test data\n",
    "\n",
    "def preprocess_test_data(X):\n",
    "    X = X.fillna(0)\n",
    "    return X\n",
    "\n",
    "def encode_cyclical(df, column, max_value):\n",
    "    # Transforming the column into its sine and cosine components\n",
    "    df[column + '_sin'] = np.sin(2 * np.pi * df[column]/max_value)\n",
    "    df[column + '_cos'] = np.cos(2 * np.pi * df[column]/max_value)\n",
    "    \n",
    "    # Optionally, you can drop the original column; depends on your preference and model's need\n",
    "    df = df.drop(column, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_test_data(X_test, location_name, features_to_convert, cat_features, nan_replacement=\"missing_value\"):\n",
    "    # Add location column\n",
    "    X_test['location'] = location_name\n",
    "    \n",
    "    # Convert specified features to int type and replace NaN values\n",
    "    for feature in features_to_convert:\n",
    "        X_test[feature] = X_test[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "\n",
    "    # Replace NaN values in categorical features\n",
    "    for feature in cat_features:\n",
    "        X_test[feature].fillna(nan_replacement, inplace=True)\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "\n",
    "def extract_datetime_features(df, column_names):\n",
    "    for column_name in column_names:\n",
    "        df[f'{column_name}_hour'] = df[column_name].dt.hour\n",
    "        df[f'{column_name}_day'] = df[column_name].dt.day\n",
    "        df[f'{column_name}_month'] = df[column_name].dt.month\n",
    "        df[f'{column_name}_year'] = df[column_name].dt.year\n",
    "        df = df.drop(columns=[column_name])\n",
    "    return df\n",
    "\n",
    "\n",
    "# 1. Preprocess test data\n",
    "X_test_a = preprocess_test_data(X_test_estimated_a)\n",
    "X_test_b = preprocess_test_data(X_test_estimated_b)\n",
    "X_test_c = preprocess_test_data(X_test_estimated_c)\n",
    "\n",
    "# 2. Extract datetime features\n",
    "columns_to_extract = ['date_calc', 'date_forecast']\n",
    "X_test_a = extract_datetime_features(X_test_a, columns_to_extract)\n",
    "X_test_b = extract_datetime_features(X_test_b, columns_to_extract)\n",
    "X_test_c = extract_datetime_features(X_test_c, columns_to_extract)\n",
    "\n",
    "#print(X_test_a.dtypes)\n",
    "\n",
    "\n",
    "# 3. Cyclically encode date-related features\n",
    "X_test_a = encode_cyclical(X_test_a, 'date_forecast_hour', 24)\n",
    "X_test_a = encode_cyclical(X_test_a, 'date_forecast_day', 30.5)\n",
    "X_test_a = encode_cyclical(X_test_a, 'date_forecast_month', 12)\n",
    "\n",
    "X_test_b = encode_cyclical(X_test_b, 'date_forecast_hour', 24)\n",
    "X_test_b = encode_cyclical(X_test_b, 'date_forecast_day', 30.5)\n",
    "X_test_b = encode_cyclical(X_test_b, 'date_forecast_month', 12)\n",
    "\n",
    "X_test_c = encode_cyclical(X_test_c, 'date_forecast_hour', 24)\n",
    "X_test_c = encode_cyclical(X_test_c, 'date_forecast_day', 30.5)\n",
    "X_test_c = encode_cyclical(X_test_c, 'date_forecast_month', 12)\n",
    "\n",
    "\n",
    "# 4. Further prepare the test data\n",
    "features_to_convert = ['is_day:idx', 'is_in_shadow:idx']\n",
    "cat_features = ['is_day:idx', 'is_in_shadow:idx', 'location']\n",
    "\n",
    "X_test_a = prepare_test_data(X_test_a, 'A', features_to_convert, cat_features)\n",
    "X_test_b = prepare_test_data(X_test_b, 'B', features_to_convert, cat_features)\n",
    "X_test_c = prepare_test_data(X_test_c, 'C', features_to_convert, cat_features)\n",
    "\n",
    "\n",
    "# 5. Filter to selected features\n",
    "\n",
    "selected_features = ['is_in_shadow:idx', 'clear_sky_energy_1h:J', 'is_day:idx', 'direct_rad_1h:J', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad_1h:J', 'diffuse_rad:W', 'sun_elevation:d']\n",
    "#selected_features = ['clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad:W', 'total_cloud_cover:p', 'sun_elevation:d', 'is_day:idx', 'is_in_shadow:idx', 'location']\n",
    "#selected_features = ['is_in_shadow:idx', 'clear_sky_energy_1h:J', 'is_day:idx', 'direct_rad_1h:J', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad_1h:J', 'diffuse_rad:W', 'sun_elevation:d', 'location']\n",
    "#selected_features = ['direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'sun_elevation:d', 'is_in_shadow:idx', 'is_day:idx', 't_1000hPa:K', 'air_density_2m:kgm3', 'relative_humidity_1000hPa:p', 'dew_point_2m:K', 'absolute_humidity_2m:gm3', 'wind_speed_v_10m:ms', 'location']\n",
    "#selected_features = ['direct_rad:W', 'direct_rad_1h:J', 'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'sun_elevation:d', 'is_in_shadow:idx', 'is_day:idx', 't_1000hPa:K', 'location']\n",
    "\n",
    "X_test_a = X_test_a[selected_features]\n",
    "X_test_b = X_test_b[selected_features]\n",
    "X_test_c = X_test_c[selected_features]\n",
    "\n",
    "\n",
    "assert all(X_test_a.columns == X_train_combined.columns), \"Mismatch in column names or order between X_test_a and X_train_combined\"\n",
    "assert all(X_test_b.columns == X_train_combined.columns), \"Mismatch in column names or order between X_test_b and X_train_combined\"\n",
    "assert all(X_test_c.columns == X_train_combined.columns), \"Mismatch in column names or order between X_test_c and X_train_combined\"\n",
    "\n",
    "\n",
    "# 6. Convert float64 columns to float32\n",
    "def convert_to_float32(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "X_test_a = convert_to_float32(X_test_a)\n",
    "X_test_b = convert_to_float32(X_test_b)\n",
    "X_test_c = convert_to_float32(X_test_c)\n",
    "\n",
    "\n",
    "\n",
    "#Testet med og uten denne, ble dårligere med..\n",
    "'''features_to_convert = ['is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "# Convert features for X_test_a\n",
    "for feature in features_to_convert:\n",
    "    X_test_a[feature] = X_test_a[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "\n",
    "# Convert features for X_test_b\n",
    "for feature in features_to_convert:\n",
    "    X_test_b[feature] = X_test_b[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "\n",
    "# Convert features for X_test_c\n",
    "for feature in features_to_convert:\n",
    "    X_test_c[feature] = X_test_c[feature].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "\n",
    "nan_replacement = \"missing_value\"\n",
    "\n",
    "# Replace NaN values in categorical features for X_test_a, X_test_b, and X_test_c\n",
    "for feature in cat_features:\n",
    "    X_test_a[feature].fillna(nan_replacement, inplace=True)\n",
    "    X_test_b[feature].fillna(nan_replacement, inplace=True)\n",
    "    X_test_c[feature].fillna(nan_replacement, inplace=True)\n",
    "\n",
    "    \n",
    "    # Convert float64 columns to float32\n",
    "def convert_to_float32(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "X_test_a = convert_to_float32(X_test_a)\n",
    "X_test_b = convert_to_float32(X_test_b)\n",
    "X_test_c = convert_to_float32(X_test_c)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_in_shadow:idx           int64\n",
      "clear_sky_energy_1h:J    float32\n",
      "is_day:idx                 int64\n",
      "direct_rad_1h:J          float32\n",
      "clear_sky_rad:W          float32\n",
      "direct_rad:W             float32\n",
      "diffuse_rad_1h:J         float32\n",
      "diffuse_rad:W            float32\n",
      "sun_elevation:d          float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#1 model predictions, catboost\n",
    "\n",
    "#print(X_test_a.iloc[:, 55])\n",
    "print(X_test_a.dtypes)\n",
    "\n",
    "\n",
    "cat_features = ['is_day:idx', 'is_in_shadow:idx', 'location']\n",
    "\n",
    "\n",
    "\n",
    "# Model predictions\n",
    "cat_prediction_a = cat_model_combined.predict(X_test_a)\n",
    "cat_prediction_b = cat_model_combined.predict(X_test_b)\n",
    "cat_prediction_c = cat_model_combined.predict(X_test_c)\n",
    "\n",
    "\n",
    "hourly_prediction_a = cat_prediction_a[::4]\n",
    "hourly_prediction_b = cat_prediction_b[::4]\n",
    "hourly_prediction_c = cat_prediction_c[::4]\n",
    "\n",
    "num_predictions = len(hourly_prediction_a)\n",
    "timestamps = pd.date_range(start='2023-05-01 00:00:00', periods=num_predictions, freq='H')\n",
    "\n",
    "# Step 1: Create individual DataFrames for each location\n",
    "df_a = pd.DataFrame({\n",
    "    'time': timestamps,\n",
    "    'prediction': hourly_prediction_a,\n",
    "    'location': 'A'\n",
    "})\n",
    "\n",
    "df_b = pd.DataFrame({\n",
    "    'time': timestamps,\n",
    "    'prediction': hourly_prediction_b,\n",
    "    'location': 'B'\n",
    "})\n",
    "\n",
    "df_c = pd.DataFrame({\n",
    "    'time': timestamps,\n",
    "    'prediction': hourly_prediction_c,\n",
    "    'location': 'C'\n",
    "})\n",
    "\n",
    "# Step 2: Concatenate these DataFrames\n",
    "final_df = pd.concat([df_a, df_b, df_c], ignore_index=True)\n",
    "\n",
    "# Step 3: Reset the index to get the 'id' column\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "# Now final_df is in the desired format\n",
    "#print(final_df)\n",
    "#print(final_df.columns)\n",
    "final_df= final_df.drop(columns=['time', 'location'])\n",
    "\n",
    "#print(final_df)\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
